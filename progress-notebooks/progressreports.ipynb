{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Project Progress Notebook (EXISTING)\n",
    "- Lindsey Rojtas\n",
    "- LING1340\n",
    "- ler75@pitt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Report 1 START (2/25/2020)\n",
    "This progress report focuses on the gathering and cleaning of data for this project; this includes filtering unnecessary columns from a list of Katakana words as well as sorting age groups in conversation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes\n",
    "The hardest part of this phase was the actual gathering of the data, especially since it's data from a language that isn't English. I wanted to do something having to do with Japanese since I think it's interesting (and hopefully, I'll have time to take some classes on it here at Pitt), but I also wanted to do something that involved having data that I could read. I'm not experienced with Kanji at all, but I can read Hiragana and Katakana. Katakana specifically interests me because it's the Japanese version of something like Italics, which makes loan words relatively easy to find.  \n",
    "\n",
    "On with the show!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Reading (Word List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the fun stuff i might need; will add to this later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had some trouble with converting this file from an Excel file to a CSV, but that's because I didn't realize I had to actually save it as a CSV rather than just renaming it. Good lesson to learn for the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = pd.read_csv('../privdata/only_katakana.csv') # reading in my big word list file... 80k entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>lForm</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>subLemma</th>\n",
       "      <th>wType</th>\n",
       "      <th>frequency</th>\n",
       "      <th>pmw</th>\n",
       "      <th>PB_rank</th>\n",
       "      <th>PB_frequency</th>\n",
       "      <th>...</th>\n",
       "      <th>LB_variable_pmw</th>\n",
       "      <th>OW_fixed_rank</th>\n",
       "      <th>OW_fixed_frequency</th>\n",
       "      <th>OW_fixed_pmw</th>\n",
       "      <th>OW_variable_rank</th>\n",
       "      <th>OW_variable_frequency</th>\n",
       "      <th>OW_variable_pmw</th>\n",
       "      <th>core_rank</th>\n",
       "      <th>core_frequency</th>\n",
       "      <th>core_pmw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>名詞-普通名詞-助数詞可能</td>\n",
       "      <td>percent</td>\n",
       "      <td>外</td>\n",
       "      <td>63392</td>\n",
       "      <td>605.970096</td>\n",
       "      <td>194.0</td>\n",
       "      <td>11614.0</td>\n",
       "      <td>...</td>\n",
       "      <td>244.656011</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6121.0</td>\n",
       "      <td>5876.767423</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28267.0</td>\n",
       "      <td>5998.526417</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>974.558557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>名詞-固有名詞-地名-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>固</td>\n",
       "      <td>29804</td>\n",
       "      <td>284.899242</td>\n",
       "      <td>388.0</td>\n",
       "      <td>6140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>243.650053</td>\n",
       "      <td>334.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>409.962374</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>429.299853</td>\n",
       "      <td>149.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>571.073098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>305</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>名詞-固有名詞-地名-国</td>\n",
       "      <td>America</td>\n",
       "      <td>固</td>\n",
       "      <td>28243</td>\n",
       "      <td>269.977496</td>\n",
       "      <td>285.0</td>\n",
       "      <td>7913.0</td>\n",
       "      <td>...</td>\n",
       "      <td>362.040945</td>\n",
       "      <td>707.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>176.658259</td>\n",
       "      <td>554.0</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>232.369421</td>\n",
       "      <td>533.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>187.625292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>351</td>\n",
       "      <td>ページ</td>\n",
       "      <td>ページ</td>\n",
       "      <td>名詞-普通名詞-助数詞可能</td>\n",
       "      <td>page</td>\n",
       "      <td>外</td>\n",
       "      <td>24642</td>\n",
       "      <td>235.555198</td>\n",
       "      <td>232.0</td>\n",
       "      <td>9624.0</td>\n",
       "      <td>...</td>\n",
       "      <td>198.520679</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>60.486252</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>36.500037</td>\n",
       "      <td>696.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>148.460790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>420</td>\n",
       "      <td>センター</td>\n",
       "      <td>センター</td>\n",
       "      <td>名詞-普通名詞-一般</td>\n",
       "      <td>center</td>\n",
       "      <td>外</td>\n",
       "      <td>20664</td>\n",
       "      <td>197.529121</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.431775</td>\n",
       "      <td>401.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>338.915030</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>335.503246</td>\n",
       "      <td>404.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>230.433005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  lForm  lemma            pos subLemma wType  frequency         pmw  \\\n",
       "0   142  パーセント  パーセント  名詞-普通名詞-助数詞可能  percent     外      63392  605.970096   \n",
       "1   286  トウキョウ  トウキョウ  名詞-固有名詞-地名-一般      NaN     固      29804  284.899242   \n",
       "2   305   アメリカ   アメリカ   名詞-固有名詞-地名-国  America     固      28243  269.977496   \n",
       "3   351    ページ    ページ  名詞-普通名詞-助数詞可能     page     外      24642  235.555198   \n",
       "4   420   センター   センター     名詞-普通名詞-一般   center     外      20664  197.529121   \n",
       "\n",
       "   PB_rank  PB_frequency  ...  LB_variable_pmw  OW_fixed_rank  \\\n",
       "0    194.0       11614.0  ...       244.656011           21.0   \n",
       "1    388.0        6140.0  ...       243.650053          334.0   \n",
       "2    285.0        7913.0  ...       362.040945          707.0   \n",
       "3    232.0        9624.0  ...       198.520679         1518.0   \n",
       "4   1118.0        2301.0  ...        55.431775          401.0   \n",
       "\n",
       "   OW_fixed_frequency  OW_fixed_pmw  OW_variable_rank  OW_variable_frequency  \\\n",
       "0              6121.0   5876.767423              19.0                28267.0   \n",
       "1               427.0    409.962374             315.0                 2023.0   \n",
       "2               184.0    176.658259             554.0                 1095.0   \n",
       "3                63.0     60.486252            2122.0                  172.0   \n",
       "4               353.0    338.915030             409.0                 1581.0   \n",
       "\n",
       "   OW_variable_pmw  core_rank  core_frequency    core_pmw  \n",
       "0      5998.526417       90.0          1070.0  974.558557  \n",
       "1       429.299853      149.0           627.0  571.073098  \n",
       "2       232.369421      533.0           206.0  187.625292  \n",
       "3        36.500037      696.0           163.0  148.460790  \n",
       "4       335.503246      404.0           253.0  230.433005  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>frequency</th>\n",
       "      <th>pmw</th>\n",
       "      <th>PB_rank</th>\n",
       "      <th>PB_frequency</th>\n",
       "      <th>PB_pmw</th>\n",
       "      <th>PM_rank</th>\n",
       "      <th>PM_frequency</th>\n",
       "      <th>PM_pmw</th>\n",
       "      <th>PN_rank</th>\n",
       "      <th>...</th>\n",
       "      <th>LB_variable_pmw</th>\n",
       "      <th>OW_fixed_rank</th>\n",
       "      <th>OW_fixed_frequency</th>\n",
       "      <th>OW_fixed_pmw</th>\n",
       "      <th>OW_variable_rank</th>\n",
       "      <th>OW_variable_frequency</th>\n",
       "      <th>OW_variable_pmw</th>\n",
       "      <th>core_rank</th>\n",
       "      <th>core_frequency</th>\n",
       "      <th>core_pmw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>84251.000000</td>\n",
       "      <td>84251.000000</td>\n",
       "      <td>84251.000000</td>\n",
       "      <td>50613.000000</td>\n",
       "      <td>50613.000000</td>\n",
       "      <td>50613.000000</td>\n",
       "      <td>24318.000000</td>\n",
       "      <td>24318.000000</td>\n",
       "      <td>24318.000000</td>\n",
       "      <td>14900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52380.000000</td>\n",
       "      <td>3291.000000</td>\n",
       "      <td>3291.000000</td>\n",
       "      <td>3291.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>11172.000000</td>\n",
       "      <td>11172.000000</td>\n",
       "      <td>11172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>96569.374215</td>\n",
       "      <td>61.648883</td>\n",
       "      <td>0.589307</td>\n",
       "      <td>65322.612451</td>\n",
       "      <td>27.371486</td>\n",
       "      <td>0.962074</td>\n",
       "      <td>31707.032939</td>\n",
       "      <td>18.169011</td>\n",
       "      <td>4.106387</td>\n",
       "      <td>20032.300872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892448</td>\n",
       "      <td>8300.864175</td>\n",
       "      <td>9.368885</td>\n",
       "      <td>8.995059</td>\n",
       "      <td>14504.842629</td>\n",
       "      <td>20.088788</td>\n",
       "      <td>4.263032</td>\n",
       "      <td>17160.913892</td>\n",
       "      <td>5.841568</td>\n",
       "      <td>5.320514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>43838.040107</td>\n",
       "      <td>446.131664</td>\n",
       "      <td>4.264615</td>\n",
       "      <td>28576.195151</td>\n",
       "      <td>147.588499</td>\n",
       "      <td>5.187552</td>\n",
       "      <td>14808.067118</td>\n",
       "      <td>68.786971</td>\n",
       "      <td>15.546578</td>\n",
       "      <td>8077.661334</td>\n",
       "      <td>...</td>\n",
       "      <td>4.255047</td>\n",
       "      <td>2921.772221</td>\n",
       "      <td>110.032822</td>\n",
       "      <td>105.642428</td>\n",
       "      <td>5375.571082</td>\n",
       "      <td>348.209277</td>\n",
       "      <td>73.893323</td>\n",
       "      <td>6831.599040</td>\n",
       "      <td>18.307100</td>\n",
       "      <td>16.674150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035149</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226011</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212209</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>61086.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>42722.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035149</td>\n",
       "      <td>19529.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226011</td>\n",
       "      <td>13724.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>6053.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>10881.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212209</td>\n",
       "      <td>12560.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>100044.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.057355</td>\n",
       "      <td>69614.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.140595</td>\n",
       "      <td>34673.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.678032</td>\n",
       "      <td>21631.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138753</td>\n",
       "      <td>8692.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.920199</td>\n",
       "      <td>16048.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>17861.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.821605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>134146.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.219859</td>\n",
       "      <td>98748.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.492083</td>\n",
       "      <td>48409.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.260105</td>\n",
       "      <td>27838.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520323</td>\n",
       "      <td>10918.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.800496</td>\n",
       "      <td>19542.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.273257</td>\n",
       "      <td>23544.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.643210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>152442.000000</td>\n",
       "      <td>63392.000000</td>\n",
       "      <td>605.970096</td>\n",
       "      <td>98748.000000</td>\n",
       "      <td>11614.000000</td>\n",
       "      <td>408.217653</td>\n",
       "      <td>48409.000000</td>\n",
       "      <td>2608.000000</td>\n",
       "      <td>589.435410</td>\n",
       "      <td>27838.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>362.040945</td>\n",
       "      <td>10918.000000</td>\n",
       "      <td>6121.000000</td>\n",
       "      <td>5876.767423</td>\n",
       "      <td>19542.000000</td>\n",
       "      <td>28267.000000</td>\n",
       "      <td>5998.526417</td>\n",
       "      <td>23544.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>974.558557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                rank     frequency           pmw       PB_rank  PB_frequency  \\\n",
       "count   84251.000000  84251.000000  84251.000000  50613.000000  50613.000000   \n",
       "mean    96569.374215     61.648883      0.589307  65322.612451     27.371486   \n",
       "std     43838.040107    446.131664      4.264615  28576.195151    147.588499   \n",
       "min       142.000000      1.000000      0.009559    194.000000      1.000000   \n",
       "25%     61086.000000      2.000000      0.019118  42722.000000      1.000000   \n",
       "50%    100044.000000      6.000000      0.057355  69614.000000      4.000000   \n",
       "75%    134146.000000     23.000000      0.219859  98748.000000     14.000000   \n",
       "max    152442.000000  63392.000000    605.970096  98748.000000  11614.000000   \n",
       "\n",
       "             PB_pmw       PM_rank  PM_frequency        PM_pmw       PN_rank  \\\n",
       "count  50613.000000  24318.000000  24318.000000  24318.000000  14900.000000   \n",
       "mean       0.962074  31707.032939     18.169011      4.106387  20032.300872   \n",
       "std        5.187552  14808.067118     68.786971     15.546578   8077.661334   \n",
       "min        0.035149    129.000000      1.000000      0.226011     84.000000   \n",
       "25%        0.035149  19529.000000      1.000000      0.226011  13724.000000   \n",
       "50%        0.140595  34673.000000      3.000000      0.678032  21631.000000   \n",
       "75%        0.492083  48409.000000     10.000000      2.260105  27838.000000   \n",
       "max      408.217653  48409.000000   2608.000000    589.435410  27838.000000   \n",
       "\n",
       "       ...  LB_variable_pmw  OW_fixed_rank  OW_fixed_frequency  OW_fixed_pmw  \\\n",
       "count  ...     52380.000000    3291.000000         3291.000000   3291.000000   \n",
       "mean   ...         0.892448    8300.864175            9.368885      8.995059   \n",
       "std    ...         4.255047    2921.772221          110.032822    105.642428   \n",
       "min    ...         0.034688      21.000000            1.000000      0.960099   \n",
       "25%    ...         0.069376    6053.000000            1.000000      0.960099   \n",
       "50%    ...         0.138753    8692.000000            2.000000      1.920199   \n",
       "75%    ...         0.520323   10918.000000            5.000000      4.800496   \n",
       "max    ...       362.040945   10918.000000         6121.000000   5876.767423   \n",
       "\n",
       "       OW_variable_rank  OW_variable_frequency  OW_variable_pmw     core_rank  \\\n",
       "count       7028.000000            7028.000000      7028.000000  11172.000000   \n",
       "mean       14504.842629              20.088788         4.263032  17160.913892   \n",
       "std         5375.571082             348.209277        73.893323   6831.599040   \n",
       "min           19.000000               1.000000         0.212209     90.000000   \n",
       "25%        10881.000000               1.000000         0.212209  12560.000000   \n",
       "50%        16048.000000               2.000000         0.424419  17861.000000   \n",
       "75%        19542.000000               6.000000         1.273257  23544.000000   \n",
       "max        19542.000000           28267.000000      5998.526417  23544.000000   \n",
       "\n",
       "       core_frequency      core_pmw  \n",
       "count    11172.000000  11172.000000  \n",
       "mean         5.841568      5.320514  \n",
       "std         18.307100     16.674150  \n",
       "min          1.000000      0.910802  \n",
       "25%          1.000000      0.910802  \n",
       "50%          2.000000      1.821605  \n",
       "75%          4.000000      3.643210  \n",
       "max       1070.000000    974.558557  \n",
       "\n",
       "[8 rows x 75 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist.describe() # stats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "What I did before anything else was rename some columns and create a `cleanwordlist` so that I only had the data I needed. 80 columns is a lot, and I don't really know what a lot of it means. Luckily, all I really need is the Katakana word, the translation, and the frequency count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist.rename(columns={'lemma':'katakana', 'subLemma':'translation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanwordlist = wordlist[['katakana', 'translation', 'frequency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>percent</td>\n",
       "      <td>63392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>America</td>\n",
       "      <td>28243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ページ</td>\n",
       "      <td>page</td>\n",
       "      <td>24642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>センター</td>\n",
       "      <td>center</td>\n",
       "      <td>20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>サービス</td>\n",
       "      <td>service</td>\n",
       "      <td>16630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>システム</td>\n",
       "      <td>system</td>\n",
       "      <td>16458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>メートル</td>\n",
       "      <td>metre</td>\n",
       "      <td>15960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>テレビ</td>\n",
       "      <td>television</td>\n",
       "      <td>15644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>メール</td>\n",
       "      <td>mail</td>\n",
       "      <td>15589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  katakana translation  frequency\n",
       "0    パーセント     percent      63392\n",
       "1    トウキョウ         NaN      29804\n",
       "2     アメリカ     America      28243\n",
       "3      ページ        page      24642\n",
       "4     センター      center      20664\n",
       "5     サービス     service      16630\n",
       "6     システム      system      16458\n",
       "7     メートル       metre      15960\n",
       "8      テレビ  television      15644\n",
       "9      メール        mail      15589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice here that some translations are `NaN`. There are several of these in the dataframe, but the first one we see is トウキョウ (Tokyo, as in the city, which has no English equivalent). Let's explore some more of this `NaN` translation data to see if the other ones are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan = cleanwordlist['translation'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>オオサカ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>センチメートル</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>キョウト</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>エド</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   katakana translation  frequency\n",
       "1     トウキョウ         NaN      29804\n",
       "11     オオサカ         NaN      11572\n",
       "25  センチメートル         NaN       9060\n",
       "30     キョウト         NaN       8495\n",
       "38       エド         NaN       8100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notranslation = cleanwordlist[isnan]\n",
    "notranslation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words read \"Tokyo\", \"Oosaka\", \"Senchimeetoru\" (centimeter), \"Kyoto\", and \"Edo\". Four of these five words are places in Japan. Centimeter isn't, but I know from looking at the list of words in the Excel spreadsheet that センチ \"senchi\" is generally used as the word for centimeter, so it makes sense that there's no translation for it. Just to be safe, let's take a random sample of non-translated words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7764</td>\n",
       "      <td>ハヤオ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20282</td>\n",
       "      <td>セネカ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>トウオウ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31856</td>\n",
       "      <td>ヤギハラ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47699</td>\n",
       "      <td>キノウチ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      katakana translation  frequency\n",
       "7764       ハヤオ         NaN         96\n",
       "20282      セネカ         NaN         24\n",
       "32900     トウオウ         NaN         10\n",
       "31856     ヤギハラ         NaN         11\n",
       "47699     キノウチ         NaN          4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notranslation.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words all seem to be names, judging by their translation and frequencies. Even if these words have meaning, I plan on filtering out words with lower frequency counts just to get rid of these names regardless, so even if I left null translations in, it wouldn't really affect what I'm trying to do here. Let's filter out the NaNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanwordlist = cleanwordlist.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>percent</td>\n",
       "      <td>63392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>America</td>\n",
       "      <td>28243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ページ</td>\n",
       "      <td>page</td>\n",
       "      <td>24642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>センター</td>\n",
       "      <td>center</td>\n",
       "      <td>20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>サービス</td>\n",
       "      <td>service</td>\n",
       "      <td>16630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  katakana translation  frequency\n",
       "0    パーセント     percent      63392\n",
       "2     アメリカ     America      28243\n",
       "3      ページ        page      24642\n",
       "4     センター      center      20664\n",
       "5     サービス     service      16630"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>30140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>113.244924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>690.539893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>63392.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "count  30140.000000\n",
       "mean     113.244924\n",
       "std      690.539893\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        9.000000\n",
       "75%       39.000000\n",
       "max    63392.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, just from getting rid of the null entries, we got rid of 50,000 words.   \n",
    "  \n",
    "Anyway, as I mentioned before, I'm going to drop any words that have an incredibly low frequency. There's no point in trying to recognize a word if there's a pretty good chance it won't be in there. The hard part here will be trying to figure out what the cutoff should be. Let's check some value counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       4484\n",
       "2       2944\n",
       "3       1937\n",
       "4       1492\n",
       "5       1286\n",
       "        ... \n",
       "1285       1\n",
       "1269       1\n",
       "3284       1\n",
       "5319       1\n",
       "2047       1\n",
       "Name: frequency, Length: 1394, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist['frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's a lot of words that are only used a single-digit amount of times. I don't want to drop too many words, but I also don't want to make this harder than it has to be. We did see above that the majority of the words were used less than 23 times, but we don't want to include anything that's too irrelevant. Let's go with dropping anything less than a frequency of 75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalwordlist = cleanwordlist.loc[cleanwordlist['frequency'] >= 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>percent</td>\n",
       "      <td>63392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>America</td>\n",
       "      <td>28243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ページ</td>\n",
       "      <td>page</td>\n",
       "      <td>24642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>センター</td>\n",
       "      <td>center</td>\n",
       "      <td>20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>サービス</td>\n",
       "      <td>service</td>\n",
       "      <td>16630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  katakana translation  frequency\n",
       "0    パーセント     percent      63392\n",
       "2     アメリカ     America      28243\n",
       "3      ページ        page      24642\n",
       "4     センター      center      20664\n",
       "5     サービス     service      16630"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalwordlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5192.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>595.130008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1576.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>63392.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "count   5192.000000\n",
       "mean     595.130008\n",
       "std     1576.927800\n",
       "min       75.000000\n",
       "25%      115.000000\n",
       "50%      202.000000\n",
       "75%      475.000000\n",
       "max    63392.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalwordlist.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered out a lot of data... we went from around 80,000 entries to around 5,000. This makes sense, considering the overwhelming amount of nulls and rarely-used words there were. If I find that I have to go back and make this more inclusive, I will. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Reading (Conversation Data)\n",
    "This part will be a little more challenging to work with. In the Nagoya Conversation Corpus, there are just text files with some annotations in them. There is data on the corpus website that specifies age groups of participants, but I'm going to need to compile a spreadsheet file myself. I'm not sure what data I'll need, so I'm importing two spreadsheets that I created; one based on the participants and one based on the text files. The participant spreadsheet probably won't need any modifying, but I'll need to import the content into the spreadsheet based on the text files. How I use these spreadsheets and whether they'll be any use at all is hard to say at this point, as I haven't actually started working with it yet, but it's a starting point. Since there aren't many text files or participants (both less than 200, creating a manual spreadsheet won't take all that long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "byparticipant = pd.read_csv('../privdata/participantbased.csv')\n",
    "byfile = pd.read_csv('../privdata/filebased.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>age</th>\n",
       "      <th>appears_count</th>\n",
       "      <th>appears_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F001</td>\n",
       "      <td>Early 20s</td>\n",
       "      <td>5</td>\n",
       "      <td>data105.txt data086.txt data076.txt data075.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F002</td>\n",
       "      <td>Late 60's</td>\n",
       "      <td>3</td>\n",
       "      <td>data033.txt data032.txt data031.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>F003</td>\n",
       "      <td>Late 80's</td>\n",
       "      <td>1</td>\n",
       "      <td>data129.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F004</td>\n",
       "      <td>Late 20's</td>\n",
       "      <td>14</td>\n",
       "      <td>data096.txt data094.txt data092.txt data082.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>F005</td>\n",
       "      <td>Late 20's</td>\n",
       "      <td>3</td>\n",
       "      <td>data052.txt data023.txt data015.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant        age  appears_count  \\\n",
       "0        F001  Early 20s              5   \n",
       "1        F002  Late 60's              3   \n",
       "2        F003  Late 80's              1   \n",
       "3        F004  Late 20's             14   \n",
       "4        F005  Late 20's              3   \n",
       "\n",
       "                                          appears_in  \n",
       "0  data105.txt data086.txt data076.txt data075.tx...  \n",
       "1                data033.txt data032.txt data031.txt  \n",
       "2                                        data129.txt  \n",
       "3  data096.txt data094.txt data092.txt data082.tx...  \n",
       "4                data052.txt data023.txt data015.txt  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byparticipant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data002.txt</td>\n",
       "      <td>F107 F023 F128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data003.txt</td>\n",
       "      <td>F033 F056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data004.txt</td>\n",
       "      <td>M018 F128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data005.txt</td>\n",
       "      <td>M023 F128 F116 M026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants\n",
       "0  data001.txt  F107 F023 M023 F128\n",
       "1  data002.txt       F107 F023 F128\n",
       "2  data003.txt            F033 F056\n",
       "3  data004.txt            M018 F128\n",
       "4  data005.txt  M023 F128 F116 M026"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on here. I don't have a lot to do with this data since we're still just in the gathering and cleaning stage, and I did lots of this manually for the sake of time and ease. There may have been a more efficient way to create the spreadsheets with code, but due to the nature of the text files, I opted for a manual spreadsheet creation.  \n",
    "  \n",
    "What I do need to do, though, is add the actual content to the `byfile` dataframe. Let's do that real quick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../privdata/nucc\\\\data001.txt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = glob.glob('../privdata/nucc/*.txt')\n",
    "fname[0] #did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def readtxt(fn):\\n    f = open(glob.glob('../privdata/nucc/' + fn)[0])\\n    text = f.read()\\n    f.close()\\n    return text\\n\\nbyfile['content'] = byfile['file'].apply(readtxt)\\n\\nbyfile.head()\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def readtxt(fn):\n",
    "    f = open(glob.glob('../privdata/nucc/' + fn)[0])\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "byfile['content'] = byfile['file'].apply(readtxt)\n",
    "\n",
    "byfile.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string I've got above here is where I'm running into an issue with encoding. Python doesn't like the fact that I have non-ASCII characters in the files and doesn't want to read them in. Whether or not I will be able to fix this before the progress report 1 deadline is really up for debate, but if I don't figure it out, I'll investigate further post-due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROGRESS REPORT 2 START (Due 3/20/2020 [extended])\n",
    "### Some notes: Responses to comments/feedback, and where I'm trying to go from here\n",
    "I'm glad I seem to have gotten a lot done in the last report! There was a lot of simple stuff going on there, though, so I'm going to take my time a bit on the next stuff since it's going to involve more than just dropping columns.\n",
    "  \n",
    "  \n",
    "Re: the columns I dropped - most of the 80 columns were just different levels of frequencies and arbitrary ranks. For now, I'm going to leave it be, but if I find I need more analysis, it won't be too lengthy of a process to go back and include some columns in the word list since I won't be doing too much manipulating for the word list. \n",
    "  \n",
    "I also freehanded the number 75 for the cutoff, but I took a look at what data I had left and saw that the words that were used 75 times or less were proper nouns that I doubt would have come up in casual conversation. The word list is really there just to find Katakana words since there aren't any word bounds in Japanese. I'd just count the number of individual Katakana characters, but onomatopoeia words are also written in Katakana and I want to avoid countig those. Again, if I need to go back and manipulate according to what my analysis needs, it isn't that lengthy of a process. \n",
    "  \n",
    "  \n",
    "I will probably just share a sample of my found data, rather than the whole thing, just to be safe regarding the licenses for the data I'm using. The code for how to get there will be public, so there will be directions to follow (or deviate from) on how to get where I got. \n",
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "`filebased` and `participantbased` are just files I created on the fly, more like an \"in case I need it later\" deal. I now know what I'm going to use them for, but I'll get into that in the next progress report. \n",
    "  \n",
    "Anyway, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the ASCII Issue\n",
    "I can't really move forward with anything until I get rid of that ASCII issue I had in the first progress report, so I'm going to start there. I'm gonna try just adding the `encoding` parameter to my function from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "      <td>＠データ１（約３５分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data002.txt</td>\n",
       "      <td>F107 F023 F128</td>\n",
       "      <td>＠データ２（６０分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data003.txt</td>\n",
       "      <td>F033 F056</td>\n",
       "      <td>＠データ０３（４３分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（某大から所属...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data004.txt</td>\n",
       "      <td>M018 F128</td>\n",
       "      <td>＠データ０４（３５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（知立駅より西...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data005.txt</td>\n",
       "      <td>M023 F128 F116 M026</td>\n",
       "      <td>＠データ０５（５５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：M023の自宅\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants  \\\n",
       "0  data001.txt  F107 F023 M023 F128   \n",
       "1  data002.txt       F107 F023 F128   \n",
       "2  data003.txt            F033 F056   \n",
       "3  data004.txt            M018 F128   \n",
       "4  data005.txt  M023 F128 F116 M026   \n",
       "\n",
       "                                             content  \n",
       "0  ＠データ１（約３５分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラ...  \n",
       "1  ＠データ２（６０分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン...  \n",
       "2  ＠データ０３（４３分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（某大から所属...  \n",
       "3  ＠データ０４（３５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（知立駅より西...  \n",
       "4  ＠データ０５（５５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：M023の自宅\\n...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readtxt(fn):\n",
    "    f = open(glob.glob('../privdata/nucc/' + fn)[0], encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "byfile['content'] = byfile['file'].apply(readtxt)\n",
    "\n",
    "byfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that worked! What a short heading. \n",
    "\n",
    "### Cleaning up the content\n",
    "The end goal here is to extract each participant's lines and put them into `participantbased` so we can analyze how many Katakana words each participant used, rather than how many appear in each file, like I first intended to. Let's look at the contents of a file, because it looks like there's some sort of documentation going on (specifying the length of the conversation as 分 is the character for minutes, the date it took place, as well as the participants). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'＠データ１（約３５分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン\\n＠参加者F107：女性３０代後半、愛知県幡豆郡出身、愛知県幡豆郡在住\\n＠参加者F023：女性４０代後半、岐阜県出身、愛知県幡豆郡在住\\n＠参加者M023：男性２０代前半、愛知県西尾市出身、西尾市在住\\n＠参加者F128：女性２０代前半、愛知県西尾市出身、西尾市在住\\n＠参加者の関係：英会話教室の友人\\nF107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁の上を歩いても１時間ぐらいですよね。\\nF023：１時間かからないぐらいだね。\\n４、５０分で。\\nF107：そうそう。\\nほいでさあ、ずっと歩いていたんだけど、そうすと上から、なんか町の中が見れるじゃん。\\nあるよね。\\nほいでさあ、なんか途中でワンちゃんに会ったんだね。\\n（ふーん）散歩をしてるワンちゃんに会ったんだ。\\nF023：城壁の上をやっぱ観光客なんだけどワンちゃん連れてきてる人たち結構多くて。\\nF107：で、こう、そのワンちゃんと２人を、なに、お父さんとお母さんと歩いて、ワンちゃんに会ったんだ。\\n途中で。\\nあワンちゃーんとか言ってなでて、ほいで、この人たちはこっち行って、あたしらこっち行ったじゃん。\\nずうーとこうやって回ってきてるの。\\nまた会っちゃって。\\nここで。\\nそうしたら。\\nF128：おー、そら地球はやっぱり丸かったみたいだね。\\u3000\\nF107：そうしたらそのワンちゃんがなんかか喜んじゃって、で、あたしの方に走ってきて、とびついてきちゃってさ。\\n別にあたしさあ、別にさっきなでただけなのにさあ、なんかすごーいなつかれちゃってね。\\u3000\\nF023：さっきね、別に、そんなになでてもいないんだよ。\\nF107：よしよしって言っただけなのに。\\nF023：あらワンちゃんだーとか言ってすれ違ったんだよ。\\n普通に。\\nそれでその次のとき、向こうの方からはーっといってかけてくるじゃん。\\nF107：すごい勢いで走って。\\n私、あ、あーさっきの犬だとか私たちが言っとるじゃん。\\nあんで向こうの人たちも、あっ、さっき会った子たちねみたいな感じで気がついたじゃん。\\n犬も気がついたじゃん。\\nじゃははって走ってきちゃって、犬が。\\nＸ：＜笑い＞そうなんだ。\\n＜笑い＞\\nF107：ほいであちしなんかとびつかれちゃったよ。\\nＸ：うそ。\\n＜笑い＞\\nF023：＊＊＊って言ってさ。\\nF107：さっきちょっとなでただけなのにって。\\nかわいかったね。\\nF023：そんでこれが２階建てのショッピングセンター。\\nすごい古いんだけど、こん中に。（おー）\\nM023：えっショッピングセンターなんですか。\\nこれ。\\nF023：そう。\\n＊５００年＊ぐらいたってるショッピングモールじゃんね。\\nM023：あ、モールなんだ。\\nF023：で、これ木でできた２階建てのとこにあって。\\nM023：で、中はモダンなんですか。\\nそれともレトロな感じで。\\nF023：中のお店は普通のショップなんだけど、床が。あのー\\nM023：木？\\nF023：木で、フロアとかが木で、で、このところの柱も木で。\\n（へー）だから\\nM023：普通にこう女性服が売ってたりとか。\\nF023：別にいろんな。\\nM023：ジーンズの店もあったとか、小物のお店もあったり。\\nF107：そう。\\nいろんな＊＊＊。\\nF023：靴、売ってたりとか。\\nF107：だから、がい、そとみだけは。なんか\\nM023：昔ながらの。\\nF107：そ、それを守って。\\n中は。\\nF128：普通になってる。\\n近代的になってる。\\nF023：ほんでなんか、昔の遺跡が出てきちゃって、ショッピングモール造ろうとしたら。\\nM023：壊せなくなったの？\\u3000\\nF023：それで壊せなかったの。\\nだから２階にショッピングモールを造ったというその歴史があって、それがこういうふうに大きな、町の中の一番大きなあのー交差点のところを強引にクロスにこういうふうに横切ってて、その２階建てのところにあって、ザ・クロスっていうんだけど、そこの交差点を。\\nで、そのショッピングモールとその建物とかが有名で、で、もともとだから古い町で、城壁が取り囲んでて、その城壁も有名で、すごいすてきな町だったよね。\\nF107：そう。\\nだからね、コンパクトで１日で見て回れるサイズで、すごいね、なんか、あ、いいすごいいい町。\\n中世の面影を残してたすごくいかわいい町だった。\\nほいでさあ、F023さんがさあ、ほいで、おもちゃ博物館に行きたいとか言ってさあ。\\nF023：＜笑い＞\\nF107：あたしおなかが痛いのにさあ、ん、いいよとか言いながらさあ、歩いて。\\nF023：私のはチェスターの町、日本で買っていったトラベルガイドだよ。\\nチェスターの町のこのクロスの中におもちゃ博物館があると。\\nそこにブリキのおもちゃのコレクションとか。\\nM023：＜笑い＞\\nF128：＜笑い＞\\nF023：ドールコレクションとかあるって。\\n（うん）私もうここさえ行ければいいって。\\nチェスターはここだけと思ってたじゃんね。\\n＜笑い＞F107と一生懸命探してさ、向こうわかーへんじゃんね。\\n通り過ぎちゃってさ、なかったよねとか言って。\\nF107：＜笑い＞うん。\\nF023：でまた通り過ぎちゃって、ないよねっとか言って。\\nF107：ほいでさ、あたしが、あっ、あそこにあるとか言ってさ、見つけたんだよね。\\nM023：それが。\\nF107：あのだからヘアードレッサーに道を、その場所を聞こうと思ったら、あっ、F023さん２階にあるじゃん。\\nここにーとかって。\\nF023：ここの２階。\\n（うーん）\\nF107：言って、見にいったら、なんか。\\nF023：トイミュージアムって書いてあったの。\\nF107：クローズド。\\nF023：クローズド。\\nM023：＜笑い＞終わってるやん。\\nF107：それでガラスの中。\\n\\u3000＜録音中断＞\\u3000\\nF023：またぷりぷりおこって、私が何でーとか言って、私このために来た。\\nF128：テレフォンカードに続く第２段。\\n＜笑い＞（へー）\\nF023：でも、すてきでしょう。\\nこここう２階もこうあって。\\n（うん、うん、うん、うん）\\nF023：下もショップなんだけど。\\n（うん、うん）\\nF023：ここの中の本屋さんが７０パー引きだった。\\nF107：そうそう。\\n７０パー引き。\\nF128：７０パー引き？本が？（びっくりしました）\\nF023：ほいでねー。\\nF128：そいで買ったんだ。\\nF023：あきに、あきに、私がバービーちゃんとか、それからドール、６０年代の人形のコレクションの写真つきのハードカバーが（ふーん）これって、裏返したらＵＳ３０ドルしてるんだね。\\nＵＳドルで。\\nF107：それがたぶん１０００円ぐらいだったね。\\n（へー）４ポンドいくら。\\nF023：えーとね３ポンド９９だもん。\\nF107：あーだもんで１０００円しない。\\n（ふーん）\\nF023：これは買うっきゃないねとかって言って。\\nF128：安いね。\\n（うん、うん）\\nF107：そうそう、すごい。\\nそれを。\\nF128：なんで閉店セールとかではなくて。\\nF023：じゃなくて。\\nF107：ほいでさあ。\\nそこの。\\nF023：本のアウトレットみたいな感じ。\\nF107：で、そこのお店がさあ。\\nF128：＜笑い＞本のアウトレット？\\u3000\\nF107：すごいちびっけで、でもそこに入るといつまでも出てこれへんもんで。\\nまずぜーんぶ見て、で、そこのお店に行ったらもう閉まっちゃう、たんだよね。\\nあたしたちが入って１５分ぐらいしたら、ダンダンダーンとかしまりしま、しまいだしちゃってさ、（うそー）あ、もだめなんだと思って、もうちょっと見たかったー。\\nF023：そうしたらこうやってドアで人がかぎ持って待ってんの。\\n私たちが出てくるのを。\\n（うそー）\\nF107：早くーとか言って。\\nF023：早く出てくれー。\\nF107：そんな感じ。\\nF023：閉店するわよって感じで。\\nF128：そうなんだー。\\nF107：でももうちょっとあそこにいたかったじゃんねー。\\nで、それが終わって、んで、戻って、ほいであのフィッシュ・アンド・チップスを食べっとって、あいであたしたちは、ほいで私はふろに入ろうと思ったらお湯が出んくって、あの人が来てくれて、で。＜笑い＞\\nF023：そういうときに調子よくさあ、大丈夫、大丈夫みたいな感じでね、ＯＫとかって、＊じゃあと＊でーとか言って。\\nF107：こうやれば出るはずなんだけど、でも出ない、出ないとか言ってさ、うーんとか言って、で、ほかのとこに行ってさ、下、なんか中２階ってか、もう１つ下がるとここにバスつきのシャワーとトイレがあって、ここはあの、全然パーフェクトにお水が、お湯が出るから、今日はあなたたちだけだから、泊り客は、よかったらここを使ってとか言われて、まあいいやあとか言ってそこを使っただよね。\\nほいで、まあ寝て、でさあ、すごいきれい、きれいというか、広くていいなと思ったら、なんか汚かっただよね、結構。\\n＜笑い＞結構ほこりがかぶってて、なーんだっていうか、もう何かこう見えるところしか掃除してない。\\n（あー）\\nF023：掃除してきちゃったもん。\\n＜笑い＞トイレのところさあ、汚かってさ、こうやって思わずふいちゃってさ。\\nきれいに。\\nわー主婦してるとか思ってさ。\\nよかったね、主婦泊めて、ここの宿とか思って＜笑い＞、バスルームまで掃除してきちゃった。\\nM023：遊びにいったのに。\\nF107：ほいでさあ、なんかさあ、トイレットペーパーも、トイレットペーパー本体がなくって。\\nF128：宿代もらわない。\\nF023：＜笑い＞\\nF107：ホルダーがなくて、ポンってそのままあったもんで、こうやって自分でちぎってさー。\\nM023：ないの？\\nこれ、切るやつが？\\nF107：そう。\\nでさ、最初さ、すごいもうお得って思ったけど。\\nM023：＊＊＊。\\nF107：やっぱこれだけ手入れしてなきゃだめだねーなんて言ってて、そこはそこでほいでよかったじゃん。\\nほいで、朝、朝食に下りてったら。\\nM023：Ｂ＆Ｂ。\\nF107：そう。\\nそうしたらさあ、だからだれも泊まってないわりにさあ、（おっさんがいっぱい）おっさんがいっぱいおっ、おっさんがいっぱいおってさあ、なにと思ったら、泊まってないって言ったよね、昨日とか言って。\\nM023：食事に下りていったっていうのはー。\\nF023：何でこんなにおっさんいっぱいいんの。\\nM023：自分でキッチンで作ったわけでじゃないんですよね。\\nF107：違う。\\n違う。\\nＢ＆Ｂなもんで出してくれるから、朝ご飯だけ。\\nM023：出してくれるね。\\nで、食べるとき、みんな食べに来てんの。\\nF107：そう。\\nM023：食事ついてるの。\\nF107：で、何時から何時までという間に朝ご飯は食べにきてって言われたじゃん。\\nほいで下りていったら、あたしが一人で下りていったら、もうすでにさあ、ふたテーブルぐらいおじさんのテーブルがあって、あ、だれも泊まってないとか言って、こんなおじさん泊まってたんだ。\\nそれにしちゃうるさくなかったね。\\n昨日静かだったななんて思ってて。\\nF023：そうだよね。\\nF107：ほいで、なんか。\\nF023：だけど、なんかテーブルがいっぱいあって、セッティングが全部してあるわけ。\\n＜笑い＞フルシートセッティングがしてあるもんで、おかしいよね、こんなに泊まってないよねとか言って、なんでこんなに用意してあるんだろうとか言って、ね。\\nF107：そしたらね、そしたら外から入ってきた。\\n＜笑い＞\\u3000もうね。\\nF128：レストランも兼みたいな感じ。\\nF107：だから朝ご飯だけ出すのかな。\\nよくわかんないけど。\\n（ふーん）ほいでなんかいろいろ聞いて、何を食べますかとか聞いとたっもんね。\\nM023：喫茶店？\\nF107：うん、喫茶店ていうか。\\nで、その人は、そこで、なんかそのお茶持ってきたりとかしとったじゃんね。\\nその昨日のお兄さんは。\\nきっと昨日のお兄さんはそれ専門だと思うじゃんね。\\n（ふーん）だからこのこっちのことを知らないのかなあ。\\nM023：でもぎ、銀行知らないのはどうかと思うよ。\\n＜笑い＞\\nF107：ほいでさー。\\nF128：それうそじゃなくて。\\nF023：ちゃんとねえ、あの＊冬＊のズボンにカッターシャツにタイをしてるんだよ。\\nM023：格好はしっかりしてるんだ。\\nF023：ビシッとしてるのよ。\\nM023：いや、日本人は、こう見かけにだまされやすい。\\nF023：ビジネスマンみたいなの。\\n格好とか。\\nだからＴシャツにジーパンとかじゃないのよ。\\nF107：ほいで何か早口なのかなんか、言っとることがよくわからんくてさあ。\\n（あー）\\nF023：よくわからんかった、私も。\\nF107：何言うとるかよくわからんかったからね。\\nM023：その辺から誤解を招いていたんじゃないの。\\n＜笑い＞\\nF107：ほいで、まそのお兄さんはそのお兄さんでいいんだけど、ほいで、また今度、朝、あのー列車に乗って新しい町に着いて、＊すとーこんとねーど＊っていって陶器の町に着いたじゃん、ね。\\nほいでまたＢ＆Ｂを探すとか言って、ガラガラガラッて、ピンポーンって、部屋空いてますか。\\n空いてません。\\nピンポーン、部屋空いてますかったら、うーん、ちょっと待ってね。\\nとにかく入りなさいとか言われてさあ。\\n（そうなんだ）そこ泊まったとこなんだけど、すごいきれいでね、いい人だった。\\nF023：かわいいきれいなとこだったの。\\nF107：どれだったっけ。\\n２だよね。\\nF023：２、２だね。\\nF107：すごいね、あのね、ワンちゃんとねえネコちゃん。\\nM023：２からもう見てないよ。\\nF107：ワンちゃんとネコちゃんがいてね、すごいかわいい、あのー、ところだったんだけど。\\nうーんと、この人をこっちに動かして、この人をこっちに動かすと、ほら１つ部屋が空いたわみたいな感じで、＜笑い＞じゃああなたたちどうぞとか。\\nM023：そうなの。\\nF023：そうそうそう。\\nだからそこもやっぱりツインじゃなくてファミリールームで４人とらま、泊まれるんだけど。\\n（ああ）\\nF107：あ、３人ね、３人。\\nF023：３つベッドが、３つベッドがあって。\\n（はいはい）\\nM023：相部屋。\\nF023：もう１つ簡易ベッドが出せるような部屋なんだけど、そこでいいかなって言って。\\nほいで、最初、F107が相部屋かって聞いたら、違う、違う。\\nあんたたちだけで使っていいからって言って。\\nF107：これがね、これが１日目のチェスターのホテル。\\nその汚い。\\nM023：見かけはきれいだけどここ、ここやどっかに。\\nF107：見かけはきれいだけど。\\n（そう、そう、そう）\\nF023：見かけはきれいだったわ。\\nF107：うん。\\nそうそうそう。\\nF023：ちょっとちょっとこうすきまを見るとほこりたまってるみたいな。\\n（へー）\\nF107：のホテル。\\nこれがチェスターの町で、＊ばっくぷらいす＊とかしてたね。\\nこれがチェスターのカシードラルの中庭。\\n（ふーん）\\nM023：ＹＨＡとかだったら自分たちで料理すればいいしさあ。（あー）\\nそれはキッチンがついているわけではない？\\nF107：じゃない。\\nF023：全然。\\nそれはＢＢというのはベッド・アンド・ブレックファストだから。\\nM023：あー、そうですね。ＢＢはそうですね。\\nF023：ベッドとブレックファストがついてる。\\nM023：俺、一回も泊まったことないんですよ。\\nＢＢ。\\nＸ：あ、そー。\\nF023：よかったよ。\\nM023：あ、そうなの。\\nＢＢの方がちょっと高いし、こぎれいなんだね。\\nF107：そうそうそう。\\nF023：でもね、ロンドン以外のＢＢは、ツインで泊まって、だから半分こすると、みんな３０００円とか４０００円以内で泊まれたよ。\\n（そう、いいね）\\nF107：あっこれこれ。\\nここのプランだけどすごくかわいいじゃんね。\\nF128：うーん。\\nかわいい。\\nF023：で、ノブとかも陶器のお花の柄のノブとかで。\\nF128：へー、かわいいね。\\nM023：俺にとってはどうかなと思うけど。\\nF023：電源のスイッチの＊にたとか＊。\\nほいでシャワーとかトイレもシェアだったんだけど、でもパッとドア開けてここにあって。\\nF107：すぐ横にあったもんね。\\n（ふーん）\\nF023：で、すごくやっぱりかわいくしてあって。\\nF107：で、あの、なに、バスタオルとかフェースタオルもおそろいのお花のやつでかわいい。\\nF023：ピンクのね。\\nすごいかわいいの。\\n（へー）\\nF107：ここはすごいかわいかった。\\n同じね、チェスターと同じ値段だもんね。\\nもう同じ値段とは思えない。\\n＜笑い＞こんなきたない宿と同じ値段とは思えないねなんて言ってね。\\nF023：F107なんかずっと私、いくつもＢ＆Ｂに泊まったけど、ここが一番きれいとかって言っちゃって。\\nM023：宿にもいっぱい泊まった＊＊＊。\\nＢ＆Ｂには一回も泊まったことない。\\nF107：ほいでね、このネコちゃんがいる。\\nF128：きれいだ。\\n美ネコだ。\\nM023：朝食ってやっぱイングリッシュ・ブレックファスト？\\nF107：そう。\\nM023：目玉焼きとウィンナー、ベーコンみたいな。\\nF107：そう。\\nで、このワンちゃんもいるんだよね。\\nF128：かわいいー。\\nF107：ほいでね。\\nF023：最初にシリアルが出て、（ああー）で、オレンジジュースが出て、シリアルが出て。\\nF107：これがさあ。\\nF023：そのあとでパンと。\\nM023：あ、そんなにいっぱい出るんだ。\\nF107：そう。\\nこれはロイヤルドルトンじゃない。\\nF023：＊＊＊。\\nF107：だもんで、さすが陶器の町なんだけど。\\nM023：そんなにいっぱい出るんだ。\\nF107：全部この。\\nM023：そんなにいっぱいあるんだ。\\nあ、でも＊＊＊はいらないな、俺。\\nF107：いい＊＊＊でもだいぶ古かったけど、それぐらい。\\nF023：＊＊＊って聞かれる。\\nそれでいいって言うとそんだけ＊＊＊。\\nF128：朝食より高そうですね。\\nだいぶね。\\nF107：あとトマトも出るんだ。\\n（ああ）\\nF023：たいがいそれだけついてるもんで。\\nM023：でハーフで一応値段が安くなるとか。\\nF107：＊＊＊という町があって、これは朝で、これはF023さんが。\\nM023：おんなじなんだ。\\n＊全部安い＊か＊＊＊って聞かれるだけや。\\nで、コーヒーか紅茶が出る？\\nF023：そうそう。\\nコーヒーと紅茶と聞かれて。\\nF128：失敗しちゃった。\\nF107：失敗して。\\nF023：ほいでこのへんとか。\\nM023：え、やっぱ紅茶うまい？\\nF128：あ、おいしい。\\nF023：１日目の宿の紅茶はうまかったよねー。\\nF107：あとはあんまおいしくなかった。\\nF023：あとはまあ普通。\\nM023：コーヒー、おいしくないよね。\\nニュージーコーヒーはもう最悪だったなあ。\\nF107：まあでもコーヒー、スターバックスもあるから。\\nM023：あ、スターバックスは。\\nF023：まあまあだったよ。\\n私の飲んだコーヒーは。\\n（へー）\\nM023：あ、ほーなんですか。\\nニュージーコーヒー、ろくな印象がないな。\\nF023：で、ティーがやっぱり安いじゃんね。\\nそれでたっぷりミルクを入れてミルク、必ずミルクティーでー。\\nF128：吉良だったかな、なんかコーヒー屋さん、吉良かな一色かな、どっち、あんねー、豊田のぐみじゃやなんか、ほんとにカップも選べて、で、もう１杯ずつ入れてくれてー、ていうお店ができたかなんか。\\nM023：吉良じゃなかった？\\nF023：できた？\\nどこなんだろう。\\nF128：うん。\\nあるとか言って。\\nなんかぐみじゃやって知ってます。\\n豊田にあるコーヒー屋さんみたい＊＊＊。\\nF107：これウィンダミヤ。\\nで、ウィンダミヤの宿も、なんかさあ、親父がさあ、お調子もんでね。\\nウィンダミヤの宿の親父がね。\\nF023：変なやつだったよね。\\nF107：変なやつだったー。\\nF023：F107のカメラに入ってる写真だからね。\\n（うーん）\\nF107：あのー、ウィンダミヤのこれは何湖だったけ。\\nコ、コ、コなんとか湖、コなんとか。\\nF023：＊こねしーらんど＊。\\nF107：うん。\\nか何かのクルーズが付いて。\\n（ふーん）これがウイリアム・シェークスピアのお墓。\\nF128：お墓？\\nF023：違う。\\nワーズワース。\\nF107：ワーズワーズだ、ウイリアム・ワーズワーズのお墓だ。\\n（ふーん）で、これがそこのウィンダミヤの＊＊＊（うんうん）じゃんね。\\nほいでさあ、次の日にＣと会うというふうで、朝、待ってたんだよね。\\nちょっとね。\\nそしたらＣとだんなさんが一緒に来てくれて、ね、でね、この日のね夜に、あたしはその前の日にチェスターで、あたしあの晩ご飯食べようって＊すとーんこーんとれんと＊か。\\nで、晩ご飯食べようと思ってパブに入ったじゃんね。\\n（うんうん）で、じさ、あたしは何にするとかって見とって、ジャケッツて書いてあったね。\\n（うんうん）あ、ジャケットポテトだと（うんうん）思って、それ食べようってあたし思ったじゃん。\\nほいで、あの、F023さんはあたしが前の日に食べたラップといって、こう、こうクルクル巻いたサンドイッチ。\\nF128：はい、はい、はい、はい、はい、はい。\\nF107：じゃ、あたしはって言ってさ、じゃあたしはジャケットポテトでじゃ、ジャケッツって書いてあって、その下にハム・アンド・チーズとかツナ・アンド・マヨネーズとか書いてあったもんで。\\nF023：何かもってこっか。\\nF107：おんなじのください。\\nF023：おんなじの。\\nF107：ください。\\nほんで、で、どれにしようかな、＊＊＊ジャケットポテト、なんかポテトにツナとかマヨネーズのやつがこうついててというのを想像したじゃんね。\\n（うん、うん、うん）\\nF107：ほんでさー、あのー、まあ、まちがっちゃうといかんと思って、わざわざメニューを指差して、これくださいって。\\n（ふーん）そうしたら向こうの人がホワイト・オア・ブラウンって言ったじゃんね。\\n白か茶色かって、何でこんなこと聞くのかなと思って。\\nF128：白パンか黒パンかってこと？\\u3000\\nM023：そう。\\nF107：うん。\\nだもんで、白パンか黒パンかって、あ、わかった、じゃあジャケットポテトの横にパンがつくんだと思って、んで、白パンか黒パンかどっちがいいか聞いてるんだと思って、じゃあ、ま、ブラウン・プリーズって言ったよね。\\nほいで、まあ自分のテーブルの番号言って待っとったんじゃん、ね。\\nそこで待ってたら、向こうから何か持ってきてさあ、ほいでさあ、来たのがね、ブラウンのパンのね、ハムサンドイッチだったの。\\n＜笑い＞はーとか思って、なんかさあ、ジャケッツってさあ、ジャケットポテ、普通は、ＤとかＣに聞いたら、（うん、うん、うん、うん）普通はジャケットポテトだって言うじゃんね。\\n（うん、うん、うん、うん、うん）で、だけど、Ｃのだんなさんだけが、あのジャケッツといってもパンのこともあるって言ったんだわ。\\nでも（うん）みんなは、だから。\\nF023：Ｄに聞いてもね、あのー、F107の友達の、あの日本人のロンドンにいる人も聞いても、それはおかしいと。\\nジャケットといったらジャケットポテトのことだよって言って。\\nみんな言ったよね。\\n（ふーん）\\nF107：パンだったじゃんね。\\n＜笑い＞ほいでさあ、そいでさあ、私さあ、その前の日におなかの調子が、胃の調子が悪くって、その冷たいサンドイッチだったじゃん。\\nで、今晩は温かいものが食べたかったじゃん。\\nそうしたらさあ、パサパサのパンが来ちゃったじゃん。\\n＜笑い＞すごいショックで、＜笑い＞あたしはジャケット。\\nF023：すごい怒るんだよ。\\nF107：ジャケットポテト。\\nF023：ぷんぷんに怒っててさ、もーとか言ってさ、ずっとコンプレインしてて。\\n＜笑い＞シーズ・コンプレイニング・オール・ザ・タイム。\\n＜笑い＞\\nF107：食べながらずっと怒って。\\nF023：ずっと怒ってた。\\nF107：パサパサパサパサしてすごい嫌だった。\\nF023：嫌だとか言って、もうほんとに嫌だ嫌だどうしてとか言ってさ、ずっと言ってて。\\n＜笑い＞\\nF107：ほいで、その日の晩はそれだったじゃん。\\nほいで、F023さんはラップサンドに温かいこういうフレンチフライがついてて、おいしそうだったじゃん。\\n私はパンだけじゃんね。\\nパサパサ。\\nF023：これあげるよって言ってさ、しょうがないからフレンチフライを食べて、怒りながら食べてさ、それも。\\nもうとか言って、こんなパサパサ、温かいものが食べたかったって言って、すごい怒って。\\nF107：ほいでもしょうがいなもんで、あのねー。\\nすごいしょうがなかったじゃんね。\\nそんなー。\\nF023：ほんでー、もう１つその前に、その、ほら、フィッシュ・アンド・チップスが山のようにあったじゃん。\\nで、あれを私はホテルで半分しか食べられなかったじゃない。\\nM023：半分食べただけでもすごいですよ。\\nF023：半分も食べられなかったんで、半分以上残ってたのね。\\nだけど主婦なもんで捨てられんくて。\\n＜笑い＞\\nF107：次の日お弁当に持ってきたら＊＊＊。\\nM023：気持ちはわかる。\\nF128：いや、私もそうしたと思う。\\nF023：だって、こんなにたくさん残ってるのに捨てられないって。\\nM023：捨てられないなあ。\\nF023：もう次の日冷たくてまずいとわかってても、私は持っていくって言って。\\nM023：次の日冷たいですよ。\\nあー、さめたポテトはね。いけてないねー。\\nいただけない。\\nF023：いただけないから。\\nま、でもしょうがないわね。\\nM023：苦しいもん。\\n食べてて苦しい。\\n＜笑い＞\\nF023：もう、そう思ってもしょうがないとか思って、そうやって持って、ビニールの袋に入れてちゃんと口を縛って、もれないように、においが。\\nＸ：＜笑い＞におうもんね。\\nF107：ほいでもさあ、でもね、食べれるところはないよって言ってたのね。\\n最初。\\n食べれるところはないと思うよって言ったけど。\\nF023：公園か何かがあったらいいねとかって言ったんだけど。\\nF107：きっとないよとか言って、でも持ってくって言うから、んーじゃあ持っていくかって言って。\\nF023：いよいよとなったら捨てればいいやとか思って。\\nF107：いざとなっ、ま、もしー食べれなかったら捨てようっていうことで持ってたんだわね。\\nM023：まだあきらめがつかないですよね。\\n食べれるかもしれない。\\n機会があるかもしれないのに。\\nF107：それでさあ、＊すとーんこんとねーど＊で。\\nF023：＊すとーんこんとねーど＊なもんで、ファクトリーショップというのに行ったのよ。\\nファクトリーショップというのは工場の直営のお店で、（うーん）ま、工場にくっついてたりしてて、だから２級品とかＢ級品とかをすごい安く売ってるお店じゃんね。\\nM023：作り損ねたやつを。\\nF023：うん。\\nでもほとんどわかんないの。\\nでも。\\nあの、素人目では何が悪いのかわからないぐらいのＢ級品じゃんね。\\nまずそのウェッジウッドのところへ行って、そしたらいい気になってこういうのを３客ずつとか買っちゃって。\\n＜笑い＞\\nF128：＜笑い＞３客。\\nF023：F107３客、私２客買ったんだ。\\nほいで、そこでずいぶん時間つぶしちゃって、で、あ、いかんいかんって、まだウェッジウッドのファクトリーも行きたいからとか言って、＜笑い＞で、次にロイヤルドルトンのファクトリーショップにもう１つ行ったのね。\\nいっぱいあるから。\\nまだミントンもあるし、スポードもあるし、すごい行きたいとこがいっぱいあって、もうしょうがないから選んだの。\\nウェッジウッドとロイヤルドルトン。\\nその２つに絞ったから、そのファクトリーショップだけは行きたかったから、で、ロイヤルドルトン行って。\\nF107：最初はさあ、ミニバスがね、そのファクトリーショップを結ぶミニ、ミニバスがあるとか書いてあったんだけど、聞いたら、そんなのは、それはもうないわよとか言われて。\\nM023：全部クローズドだ。\\nF128：＜笑い＞全部ないんだ。\\nF107：それで、ミニバスはないわよ。\\n自分でこのバスで行くのよとか言って、でもそのバスがオールデイ・チケットというのがあって、あのー、なんか３ポンドぐらい出すと乗り放題なのね。\\n（へー）\\nF023：すごい安いじゃんね。\\nF107：で、あの、乗ったり降りたりして、乗り放題のそのチケットがあって。\\nF023：丸１日全部使って。\\nF107：でもさあ、そのチケットがさあ、なんかさーなんかちゃんとしっかりしてなくって、レシートみたいなチケットだもんね。\\nF023：レシートなの、レシート。\\nF107：ここに入れとるとだんだんだんだん薄くなってきちゃってさあ。\\nF023：運転手さんがその場でワンデイ・チケットでとか言うと、こうやってカチャカチャカチャって、これくらいの機械でカチャカチャカチャってやって、で、ピーって出てきたレシートをビリッと破って、はいってくれるじゃんね。\\nしかも２人分ねってって１枚じゃんね。\\n＜笑い＞（はー）\\nF107：これ、なんかだんだんよぼよぼになってきちゃって、最後の人なんかはあってしとったよね。\\n最後は。\\n夜遅かったしさあ。\\nF023：本当にワンデイ・チケットかみたいな感じで確認されちゃったんだけどー。\\nF107：＜笑い＞最後なんか笑えるよね。\\n最後さあ、ほんでさあ、もうこれで駅ま、駅の近くになったら押そうねとか言って、もうだいぶ慣れてきたも駅はわかるから、駅の手前になったら押せるじゃん。\\nほいでさあ、私たちなんか油断しとったんだよね。\\nだれか駅で降りるだろうと思って、駅、あ、もうあとちょっとで駅だよねなんて話してたじゃん。\\nで、押すっていう行動が頭の中になかったじゃんね。\\n駅は止まるもんだと思っとったじゃん。\\n（はーはーはーはー）勝手に。\\nF023：だれか降りる、だれか乗る。\\n（あー）\\u3000\\nF107：だもんで止まるもんだと思っとって、（ねえ）そしたらさあ、そしたらだよ。\\nM023：＊＊＊。\\nF107：通過しちゃったじゃんね。\\nほいで、ちょっと押してとか言って押したら、止まってくれへんだよね。\\nあれ。\\nF128：それは止まれんのは駅じゃないからでしょう。\\nF107：違う。\\nだから駅がちょっと過ぎたぐらいで押したじゃん。\\n降ろしてほしいじゃん。\\n私たちとしては。\\nM023：次の駅で止まるね。\\n＜笑い＞\\nF107：ほいで、その次の駅がすっごい遠くてね。\\n＜笑い＞\\nF023：あ、というか、カーブになっとって、で、カーブだけならいいんだけど、あの、高架の下をガーってくぐって、大きな道路の向こう側に出て。\\nF107：ほで橋渡ってさあ。\\nF023：橋渡って、しかも＊＊＊もう１つは大きい道路をガーって曲がってって。\\nM023：時間が迫ってるときは遠く感じるよ。\\n＜笑い＞\\nF107：す、で、さあ、何で降ろしてくれへんていう感じじゃない。\\n私たちとしては。\\nすごい遠かったんだよね。\\nF023：で、なんか\\nF107：夜も遅いしさ\\nF023：あれっどうしよう。\\nF107：ど、ど、どうしようとか思って。\\nF023：それでとりあえず次のとこで降りてー。\\nF107：だから信号、だから信号待ちで降ろしてもらおうと思ったんだけど、右曲がり車線だもんで、道路のど真ん中で降りることになっちゃうんじゃん。\\n＜笑い＞それはね、やっぱりいくらなんでも言えんかったじゃんね。\\nM023：やっぱりね、降ろしてくれとはね。\\nF107：じゃあまあ次の駅までとか思って、F023さんがね、ね次の駅ってどこって私が聞いたら、あそこ曲がったあの辺じゃないって言うから、そこら辺だと思っとったら、何かだあーって下りる方とこっちに行く方があって、F023さんはここら辺じゃないって言ったじゃんね。\\nM023：ああ、下りた。\\nF107：ったらどおーっと下りてて。\\nあ。\\nF023：曲がっちゃったって。\\nF128：曲がっちゃったって。\\nF023：ああ下りちゃった、ああくぐっちゃった。\\n＜笑い＞\\u3000\\nF107：ほいで、すごい。\\nF023：＊＊＊てて、２人でね、バスの中であたふた、どこまで行っちゃうとか言って、＊＊＊まだ行くよとか言って。\\n＜笑い＞\\nM023：どこまで行くって言えばよかった。\\nそのとき。\\nF107：ていうかね、あのね、いつもは言うんだけど、慣れてる、もう駅はわかるから、駅の近くはわかるからっていう頭があって油断しとったじゃんね。\\n（あー）（あー、そうか、そうか）\\nF023：だれか押すと思ったんだよね。\\nF107：ほいで、押しゃあいいのに押しゃあへんかっただよね。\\nF128：＜笑い＞押そうよ。\\nF023：そいでさ、駅前来ちゃったから押さなきゃ押さなきゃって言って押したときは、駅のちょうど真正面をちょっと通過したとこだったの。\\nほいで止まってくれると思ったら、そのままバアー。\\nF128：で、やっちゃった。\\nF023：止めてって言やよかったけど、そのときにね。\\n＜笑い＞でもさ、すぐ止まるすぐ止まるとかって思って。\\nF107：勝手に思って、すぐ止ま、だか、その前がちょんちょん止まってたもんで、すぐ止まると思ったんだ。\\nだから、その駅の前もちょんちょんと止まってたから距離が少ないと、思っと、勝手に思い込んでたら、駅からあとがすごい距離が長かったじゃんね。\\nほいで、降りて反対側に行くバス探そうと思ったら、なんか暗いし、バス停がどこにあるかわからへんかったじゃんね。\\nM023：歩いちゃった。\\nF128：歩いちゃったんだ。\\nF023：そう。\\nで、反対側のバス停が見つからんし、待ってる間に歩けるかもねとかって言って、歩いて帰ろう＊＊＊。\\nM023：ある意味で男の旅じゃん。\\nそういうのって。\\n僕なんかやりますよ。\\nそういうの。\\nF107：歩いちゃうの？\\nM023：歩いちゃうっていうか、どこへ行くかわからんけど、つられてバスに乗っちゃったとかさ。\\nF023：だけど、そう遠くなかった。\\n歩いたら。\\nドキドキしてるから歩いたら結構すぐに高架が＊＊＊。\\nM023：失敗した。\\n＊＊＊やるけど、＊＊＊。\\n英語もしゃべれて、大の大人２人が。\\nふつうね、女の人の２人旅ですからちょっとぜいたくな感じがしてね。\\nF023：＊＊＊する。\\n駅まで行かなくてもＢＢの正面に出てたからかえって。\\nこっちからだと、もちろんこっちの方が遠いんだけど、（あー、あー、あー、あー）ＢＢには近いところの道に出れたんで。\\nM023：＊＊＊たいなあ。\\nF107：ほんで、ＢＢに戻って、ほいで、部屋に入って。\\nF023：その前に、だからあたしはロイヤルドルトンでお昼を食べたの。\\nフィッシュ・アンド・チップスの残り。\\n＜笑い＞（食べた）ロイヤルドルトン入っていったら、ちょうど喫茶コーナーみたいのがあって、こうベンディング・マシーンが何台か置いてあって、テーブルが置いてあって、で、ここでコーヒー飲んでくださいねってとこがあったの。\\nやったーとかって、で、そこでごそごそごそごそ広げて。\\nF128：広げてー。\\n＜笑い＞\\nF023：広げて、食べて。\\n２人で食べたよね。\\nでも。\\nで、お昼を食べる時間も場所もなかったの。\\nそのときには。\\nもうお昼過ぎてたの。\\nそのときには。\\nそれで、そのあとすぐウェッジウッドのファクトリー行きたかったからー、ちょうどいいじゃん。\\nで、F107はすごい渋々なのね。\\n＜笑い＞\\nF107：そんなの恥ずかしいから。\\nF128：恥ずかしい。\\n恥ずかしい。\\nM023：冷えたポテトだもんね。\\nそら。\\nF107：恥ずかしいし、ほいで、食べてたらー、なんかそのショップの女の人が、まあこんなところであのー、お、なんかチップス食べてておいしそうねえ、私も欲しいわとか言ってたけど、でも。\\nM023：絶対欲しくないよね。\\n＜笑い＞\\nF107：冷たいとは知らんかったと思う。\\n＜笑い＞\\nF023：調子いいなと思ってたと思うけど、でもみんな優しかったよ。\\nわあうらやましいとか、おいしそうねとかって、みんなにこやかだった。\\nM023：ああー、＊＊＊だ。\\nF023：うん。\\nただ、あ、こんなとこでとか思ったけど、でもしょーうがないと思ってたんだろうね。\\nあれね。\\nきっとね。\\nM023：あったかければね、絶対おいしいんだけど。\\nF023：やあねえ、ロイヤルドルトンだよ。\\n王室、皇室ご用達の＜笑い＞＊かま＊のさあ、物を売ってる店でさあ、日本人が２人でさあ、何かごそごそ。\\nM023：で、晩ご飯はどうしたんですか。\\nF107：晩ご飯は。\\nF023：だから晩ご飯がそれよ。\\nジャケット。\\nM023：あー、それで、ふだん、ふだん、ふだん、ふだん。\\nF107：ふだんはそこら辺で食べたよ。\\nF023：ねえ。\\nM023：いいレストラン？\\u3000\\nF107：それがさあ、あのさあ、ウィンダミヤのさあ、レストランが最低だった。\\n＜笑い＞\\nM023：まずい？\\nF107：だからその前の日のジャケッツであたしは懲りているわけよ。\\n（あー）もう今日こそはあったかいものが食べたいと。\\n（あー）\\nF023：そう。\\nで、パスタが食べたいとかって思って。\\nF107：で、あのー、なんだ、パスタが食べたいって言ったら、パスタどれくらいって言ったら、３０分歩いたらかかるって言うのね。\\nだからー、ちょっと遠いじゃん。\\nで、タクシーだったらー４ポンドくらいで行けるって言うんだけど、まあ、ちょっと町も歩きたかったし、ちょっとフラフラ歩きながら途中でいい店があったら。\\nM023：あったらそこに入ってみようかと。\\nF107：入っちゃおうかと思いつつ行ったじゃん。\\nほいでさあ、２つこう道がこうあったんだけど、ほで１つにこうなってこっちの方にあるじゃん。\\nほいで歩いていって、で、こっちの方が短そうだったもんで、F023さんがこっちを行こうって言ったじゃん。\\nほんでこっちを行って、こうやって行って、ここら辺に１つお店があったもんで、まあここにしちゃうかって話になって入ったんだわ。\\nほんでさあ、こうやって見てて、なんかないかなあなんて見てて、最初はなんか何食べようかなとか言って、なんか食べようと思ったんだけど、あ、そうだ、で、カレーライスがあったんだわ。\\nポーク。\\nF023：チキンカレーがあったの。\\nM023：日本のカレーとも違いますよね。\\nF107：わあーカレーライス食べようと思って、あったかいしと思って。\\nF023：ご飯が食べれるとか思って。\\nF107：で、頼んだらさあ、もうえらいことでさあ。\\nもうなんか。\\nF023：ひどい。\\nF107：お米がさあ。\\nF023：まずい、まずい。\\nM023：インディカ米？\\u3000\\nF107：洗ったような米。\\nなんかね普通の。\\nF023：水浸し。\\nM023：聞くだけで嫌だな。\\nF107：そう。\\nF023：パサパサで。\\nM023：あったかくないんでしょう。\\nF023：硬くて、下に水がたまってるの。\\n（えー）\\nF107：こういうふうに丸く、あの、こういうふうにあの。\\nF023：ライスが。\\nF107：ライスがこうやって真ん中にルーが入ってるの。\\nF023：ここにずうっとあって、で、ここにカレーがダーっと入ってて。\\nF107：ルーが入って。\\n見た目はうまそうなんだけど。\\nF128：おいしくないんだ。\\nF107：そのご飯が、なんていうの、いったんゆでて。\\nF023：ゆでて。\\nF107：なんかすごく軟らかくならないうちに水を切って、水で洗っちゃったっていうような。\\nM023：＜笑い＞最高だね。\\n食べたくもないなあ。\\nF023：結局水をよく切らないうちに、ゆでて、で、ゆできらないうちに上げて、水がよく切れないうちに。\\nだあって＊＊＊。\\nだから水がだんだん下に染みだしてきて、下にダーと透明な水がたまってるじゃんね。\\nF128：えー、おいしくなさそう。\\nM023：米のたき方知らないよね。\\nF107：もうすごいまずくて、もうなんか、それでも。\\nF023：それでも＊＊＊食べて＊＊＊。\\n＜笑い＞\\nF107：ご飯がよかったもん。\\nだって。\\nご飯が食べたくって、まずかったんだけどそれでも食べたんだわ。\\nF023：カレー自体、カレー自体は割とよかった。\\nM023：スーパーマーケットとか行きました？\\u3000\\nF107：行ったよ。\\nM023：楽しいんじゃないですか。\\nありますよね。\\nニュージーでもいっぱいある。\\nスーパーマーケットは、い、楽しい？\\u3000\\nF107：楽しかったね。\\nM023：一番好きだな。\\nF107：うん。\\nそれでさあ、ウィンダミヤで、ほんで今度はＣに次の日に会っていろいろ連れていってもらったんだけど、大雨じゃんね。\\nすごい雨だったよね。\\nほんでＣと会って、雨で、で、Ｃが、あのー、写真を見せてくれたんだけど、結婚式の。\\nウェディングドレスじゃないじゃんね。\\nF128：あ、そうなんだ。\\nF107：普通の服だよね。\\n普通のってか、ちょっとおしゃれな。\\nF023：シルクサテンみたいな光沢のあるスーツなの。\\n（ふーん）\\nF128：帰る？\\nF107：帰る？\\nF128：寝そうだよ。\\n＜笑い＞\\u3000\\nF107：ほんとだよね。\\nあれ。\\nF023：グレーっぽいようなスーツだった。\\n普通の。\\n（ふーん）それで頭きれいにアップしてるんだけど、ほいで花束持ってるんだけど、それだけなのね。\\nF128：いつ、それ。\\nF107：うんとね４月って言ってた。\\n（ふーん）ほいでさあ、あのー、写真を持って見せたげるって言ったんだけど１枚しかなくてさあ。\\nF023：私、探したんだけどこれしかないのよって言って。\\nF107：で、お母さんのところにあって、なんか電話をしたらお父さんしかいなくて、お母さんしかどこにあるかわからないから持ってこられなかったのって言うじゃんね。\\nで、私たちはそれは何ウェディングドレ、ドレスを着てるんだと思っとったわけ。\\nで、それはなんていうの。\\nウェディングドレスを着る前のお写真とか、ウェディングドレスを脱いだあとのパーティーのドレスだと思ってたわけよ。\\nF023：パーティーのときのだと思ったわけね。\\nF107：だから。\\nF128：ドレス見にいくって言ってたでしょう。\\nF107：そう、そう、そう、そう。\\nだからほかの写真も見たいって言ったじゃん。\\nだからそれがウェディングドレスだとは思ってないもんで、ほかにレースだとか見れるかと思って、ほかのが見たいって言ったら、お母さんのところで持ってこれなかったって。\\nF023：＊＊＊に会いたいとかって思ってたじゃん。\\nずっと。\\nお父さんに会いたいとかって。\\n行っちゃうとか言って、じゃお母さんのところで残りの写真見れないとかって、いっちゃうとかって言って。\\nそして行ったら。\\nM023：帰る。\\nこんな、こんな遅くまで起きてたことない。\\n俺。\\nＸ：はい。\\nF107：F128は戻ってくる？\\nF128：戻ってくる。\\nM023：これ＊＊＊ずっと続いとる。\\n何分くらいなんの。\\nF128：１時間。\\nまだ５０分くらい。\\nM023：十分だ。\\nF107：十分だよね。\\nF128：２本目に。\\nF023：お疲れー。\\nM023：もう眠い。\\nこんな遅くまで起きてたことない。\\nF023：ほんと？\\nM023：いつも夢の中。\\nF023：明日があるもんね。\\nF107：これさあ、なんか、なんかに入れてさあ、くれると、あれできるんだけどな。\\nF023：これね、ネガポジで見てチェックしてくれればいいよ。\\nF107：そうだね。\\nF023：そしたら焼き増ししやすいから。\\nF107：あ、でもそれ一回さあ、私もさあ、＊＊＊。\\nF023：＊＊＊。\\n\\u3000＜テープ反転＞\\nF023：どれがあって、どれがない写真かというのがわからんでしょう。\\nF107：わからん。\\nF023：景色のやつなんかさあ。\\nF107が撮るならいいわとか言ってたのん結構あったもんね。\\nF107：そうだね。\\nじゃあいったん私、持ってきてからにしようか。\\nF023：うん。\\nで、最低３枚も。\\nF107：ほんとだよね。\\nF023：これが撮りたかったんだけど、暗かった。\\n向こうの方だったから。\\nF107：暗かったねー。\\nF023：うん。\\nフラッシュはたけたんだよ。\\nこのとき。\\nでも遠くだったもんで。\\nこれ自身は。\\nF107：これすごい＊＊＊ない。\\nF023：うん。\\nそうねえ。\\n朝早くから悪かったねー。\\nF107：ほんとだよね。\\nF023：＊＊＊立派でさかわいいの。\\nもうーほんとに。\\nコーギーのしっぽは切らなくていいわあ。\\nF107：かわいいねえ。\\nF023：この子もなんかなつっこかったよねえー。\\nF107：この子ね、ね、毛、毛が抜け抜けだったよ。\\nでも。\\nでもこの部屋はかわいかった。\\nF023：すごいきれいにしてあったね。\\nF107：＊おさむらい＊。\\nこれ反対になっちゃったねー。\\nF023：ねー。\\nF107：なんで暗くなっちゃったのかなあ。\\nF023：これ、あのね、たけなかったの。\\nフラッシュが。\\nF107：あ、それでか。\\nF023：ここが明るかったからあ。\\nだからフラッシュがたけなかった。\\nF107：これ写せたね。\\nというかさあ、でもさ、全然、さ、全然これがうまくいってないね。\\nF023：というか、顔が変だし、私たち。\\n＜笑い＞ハーとか言って。\\nF107：これさあ、フラッシュがたけてないのも＊＊＊よかったね。\\nF023：やっぱり部屋が明るかったんだね。\\nたけたり、たけなかったりしたけどさ、全部写ってたからさあ。\\nF107：そうだね。\\u3000\\nF023：おもしろかったね。\\nこの人もねえ。\\nF107：洋食が嫌いとか言っちゃってさあ。\\nF023：彼女と一緒にトイレットとか行っちゃって。\\n＜笑い＞やーだーとか。\\nF107：でもさあ、これ写真で見るとさ、こんなちょっとだったと思うよ。\\nF023：そうなの。\\nものすごい撮ったと思って、まだでもね、十何枚入ってるから。\\nカメラの中に。\\nそれをまた焼けばもうちょっとあるんだけどね。\\nF107：これきれいに撮れているじゃん。\\u3000\\nF023：ねえ。\\nほんと。\\nF107：おいしそう。\\nパブの中。\\nＸ：１：＊＊＊。\\nF107：これもすごくきれいに撮れてるじゃん。\\n＊＊＊。\\n＜間＞これは＊＊＊のロンドン塔だよね。\\nF023：＊＊＊。\\nF107：そうだねえ。\\nだってこんなふうに撮れると思わんかったもんね。\\nF023：そう、そう、そう。\\n＊＊＊よりもすてきだよね。\\n元シティホールだったって書いてあったよ。\\nF107：私どこを見てるんだろう。\\n＜笑い＞\\nF023：これは何ポリスっていうの。\\nF107：騎馬隊。\\nF023：騎馬って＊＊＊。\\nF107：＊とうしば＊。\\nF023：違う。\\nF107：なんだろうね。\\nF023：なんだろうね。\\n＜間＞ここまでしかあれができなかったからさ。\\nF107：そうだね。\\nF023：＊＊＊。\\nこれちょっとすてきだよね。\\nF107：そうだね。\\nF023：これちょっとすてきに撮れた。\\nなんかこれシャッターチャンスがさ、逃すん＜笑い＞だよね。\\n遅いもんでさあ。\\n反応が。\\n＠ＥＮＤ\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byfile['content'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not pleasant to look at in the slightest. Any line beginning with an @ symbol is documentation, so let's find a way to remove that first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
