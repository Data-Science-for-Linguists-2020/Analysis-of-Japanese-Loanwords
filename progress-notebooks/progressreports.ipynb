{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Project Progress Notebook (EXISTING)\n",
    "- Lindsey Rojtas\n",
    "- LING1340\n",
    "- ler75@pitt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Report 1 START (2/25/2020)\n",
    "This progress report focuses on the gathering and cleaning of data for this project; this includes filtering unnecessary columns from a list of Katakana words as well as sorting age groups in conversation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes\n",
    "The hardest part of this phase was the actual gathering of the data, especially since it's data from a language that isn't English. I wanted to do something having to do with Japanese since I think it's interesting (and hopefully, I'll have time to take some classes on it here at Pitt), but I also wanted to do something that involved having data that I could read. I'm not experienced with Kanji at all, but I can read Hiragana and Katakana. Katakana specifically interests me because it's the Japanese version of something like Italics, which makes loan words relatively easy to find.  \n",
    "\n",
    "On with the show!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Reading (Word List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the fun stuff i might need; will add to this later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had some trouble with converting this file from an Excel file to a CSV, but that's because I didn't realize I had to actually save it as a CSV rather than just renaming it. Good lesson to learn for the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = pd.read_csv('../privdata/only_katakana.csv') # reading in my big word list file... 80k entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>lForm</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>subLemma</th>\n",
       "      <th>wType</th>\n",
       "      <th>frequency</th>\n",
       "      <th>pmw</th>\n",
       "      <th>PB_rank</th>\n",
       "      <th>PB_frequency</th>\n",
       "      <th>...</th>\n",
       "      <th>LB_variable_pmw</th>\n",
       "      <th>OW_fixed_rank</th>\n",
       "      <th>OW_fixed_frequency</th>\n",
       "      <th>OW_fixed_pmw</th>\n",
       "      <th>OW_variable_rank</th>\n",
       "      <th>OW_variable_frequency</th>\n",
       "      <th>OW_variable_pmw</th>\n",
       "      <th>core_rank</th>\n",
       "      <th>core_frequency</th>\n",
       "      <th>core_pmw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>名詞-普通名詞-助数詞可能</td>\n",
       "      <td>percent</td>\n",
       "      <td>外</td>\n",
       "      <td>63392</td>\n",
       "      <td>605.970096</td>\n",
       "      <td>194.0</td>\n",
       "      <td>11614.0</td>\n",
       "      <td>...</td>\n",
       "      <td>244.656011</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6121.0</td>\n",
       "      <td>5876.767423</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28267.0</td>\n",
       "      <td>5998.526417</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>974.558557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>名詞-固有名詞-地名-一般</td>\n",
       "      <td>NaN</td>\n",
       "      <td>固</td>\n",
       "      <td>29804</td>\n",
       "      <td>284.899242</td>\n",
       "      <td>388.0</td>\n",
       "      <td>6140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>243.650053</td>\n",
       "      <td>334.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>409.962374</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>429.299853</td>\n",
       "      <td>149.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>571.073098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>305</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>名詞-固有名詞-地名-国</td>\n",
       "      <td>America</td>\n",
       "      <td>固</td>\n",
       "      <td>28243</td>\n",
       "      <td>269.977496</td>\n",
       "      <td>285.0</td>\n",
       "      <td>7913.0</td>\n",
       "      <td>...</td>\n",
       "      <td>362.040945</td>\n",
       "      <td>707.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>176.658259</td>\n",
       "      <td>554.0</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>232.369421</td>\n",
       "      <td>533.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>187.625292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>351</td>\n",
       "      <td>ページ</td>\n",
       "      <td>ページ</td>\n",
       "      <td>名詞-普通名詞-助数詞可能</td>\n",
       "      <td>page</td>\n",
       "      <td>外</td>\n",
       "      <td>24642</td>\n",
       "      <td>235.555198</td>\n",
       "      <td>232.0</td>\n",
       "      <td>9624.0</td>\n",
       "      <td>...</td>\n",
       "      <td>198.520679</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>60.486252</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>36.500037</td>\n",
       "      <td>696.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>148.460790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>420</td>\n",
       "      <td>センター</td>\n",
       "      <td>センター</td>\n",
       "      <td>名詞-普通名詞-一般</td>\n",
       "      <td>center</td>\n",
       "      <td>外</td>\n",
       "      <td>20664</td>\n",
       "      <td>197.529121</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.431775</td>\n",
       "      <td>401.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>338.915030</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>335.503246</td>\n",
       "      <td>404.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>230.433005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  lForm  lemma            pos subLemma wType  frequency         pmw  \\\n",
       "0   142  パーセント  パーセント  名詞-普通名詞-助数詞可能  percent     外      63392  605.970096   \n",
       "1   286  トウキョウ  トウキョウ  名詞-固有名詞-地名-一般      NaN     固      29804  284.899242   \n",
       "2   305   アメリカ   アメリカ   名詞-固有名詞-地名-国  America     固      28243  269.977496   \n",
       "3   351    ページ    ページ  名詞-普通名詞-助数詞可能     page     外      24642  235.555198   \n",
       "4   420   センター   センター     名詞-普通名詞-一般   center     外      20664  197.529121   \n",
       "\n",
       "   PB_rank  PB_frequency  ...  LB_variable_pmw  OW_fixed_rank  \\\n",
       "0    194.0       11614.0  ...       244.656011           21.0   \n",
       "1    388.0        6140.0  ...       243.650053          334.0   \n",
       "2    285.0        7913.0  ...       362.040945          707.0   \n",
       "3    232.0        9624.0  ...       198.520679         1518.0   \n",
       "4   1118.0        2301.0  ...        55.431775          401.0   \n",
       "\n",
       "   OW_fixed_frequency  OW_fixed_pmw  OW_variable_rank  OW_variable_frequency  \\\n",
       "0              6121.0   5876.767423              19.0                28267.0   \n",
       "1               427.0    409.962374             315.0                 2023.0   \n",
       "2               184.0    176.658259             554.0                 1095.0   \n",
       "3                63.0     60.486252            2122.0                  172.0   \n",
       "4               353.0    338.915030             409.0                 1581.0   \n",
       "\n",
       "   OW_variable_pmw  core_rank  core_frequency    core_pmw  \n",
       "0      5998.526417       90.0          1070.0  974.558557  \n",
       "1       429.299853      149.0           627.0  571.073098  \n",
       "2       232.369421      533.0           206.0  187.625292  \n",
       "3        36.500037      696.0           163.0  148.460790  \n",
       "4       335.503246      404.0           253.0  230.433005  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>frequency</th>\n",
       "      <th>pmw</th>\n",
       "      <th>PB_rank</th>\n",
       "      <th>PB_frequency</th>\n",
       "      <th>PB_pmw</th>\n",
       "      <th>PM_rank</th>\n",
       "      <th>PM_frequency</th>\n",
       "      <th>PM_pmw</th>\n",
       "      <th>PN_rank</th>\n",
       "      <th>...</th>\n",
       "      <th>LB_variable_pmw</th>\n",
       "      <th>OW_fixed_rank</th>\n",
       "      <th>OW_fixed_frequency</th>\n",
       "      <th>OW_fixed_pmw</th>\n",
       "      <th>OW_variable_rank</th>\n",
       "      <th>OW_variable_frequency</th>\n",
       "      <th>OW_variable_pmw</th>\n",
       "      <th>core_rank</th>\n",
       "      <th>core_frequency</th>\n",
       "      <th>core_pmw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>84251.000000</td>\n",
       "      <td>84251.000000</td>\n",
       "      <td>84251.000000</td>\n",
       "      <td>50613.000000</td>\n",
       "      <td>50613.000000</td>\n",
       "      <td>50613.000000</td>\n",
       "      <td>24318.000000</td>\n",
       "      <td>24318.000000</td>\n",
       "      <td>24318.000000</td>\n",
       "      <td>14900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52380.000000</td>\n",
       "      <td>3291.000000</td>\n",
       "      <td>3291.000000</td>\n",
       "      <td>3291.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>11172.000000</td>\n",
       "      <td>11172.000000</td>\n",
       "      <td>11172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>96569.374215</td>\n",
       "      <td>61.648883</td>\n",
       "      <td>0.589307</td>\n",
       "      <td>65322.612451</td>\n",
       "      <td>27.371486</td>\n",
       "      <td>0.962074</td>\n",
       "      <td>31707.032939</td>\n",
       "      <td>18.169011</td>\n",
       "      <td>4.106387</td>\n",
       "      <td>20032.300872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892448</td>\n",
       "      <td>8300.864175</td>\n",
       "      <td>9.368885</td>\n",
       "      <td>8.995059</td>\n",
       "      <td>14504.842629</td>\n",
       "      <td>20.088788</td>\n",
       "      <td>4.263032</td>\n",
       "      <td>17160.913892</td>\n",
       "      <td>5.841568</td>\n",
       "      <td>5.320514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>43838.040107</td>\n",
       "      <td>446.131664</td>\n",
       "      <td>4.264615</td>\n",
       "      <td>28576.195151</td>\n",
       "      <td>147.588499</td>\n",
       "      <td>5.187552</td>\n",
       "      <td>14808.067118</td>\n",
       "      <td>68.786971</td>\n",
       "      <td>15.546578</td>\n",
       "      <td>8077.661334</td>\n",
       "      <td>...</td>\n",
       "      <td>4.255047</td>\n",
       "      <td>2921.772221</td>\n",
       "      <td>110.032822</td>\n",
       "      <td>105.642428</td>\n",
       "      <td>5375.571082</td>\n",
       "      <td>348.209277</td>\n",
       "      <td>73.893323</td>\n",
       "      <td>6831.599040</td>\n",
       "      <td>18.307100</td>\n",
       "      <td>16.674150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035149</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226011</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212209</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>61086.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>42722.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035149</td>\n",
       "      <td>19529.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.226011</td>\n",
       "      <td>13724.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>6053.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960099</td>\n",
       "      <td>10881.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212209</td>\n",
       "      <td>12560.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>100044.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.057355</td>\n",
       "      <td>69614.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.140595</td>\n",
       "      <td>34673.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.678032</td>\n",
       "      <td>21631.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138753</td>\n",
       "      <td>8692.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.920199</td>\n",
       "      <td>16048.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>17861.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.821605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>134146.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.219859</td>\n",
       "      <td>98748.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.492083</td>\n",
       "      <td>48409.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.260105</td>\n",
       "      <td>27838.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520323</td>\n",
       "      <td>10918.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.800496</td>\n",
       "      <td>19542.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.273257</td>\n",
       "      <td>23544.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.643210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>152442.000000</td>\n",
       "      <td>63392.000000</td>\n",
       "      <td>605.970096</td>\n",
       "      <td>98748.000000</td>\n",
       "      <td>11614.000000</td>\n",
       "      <td>408.217653</td>\n",
       "      <td>48409.000000</td>\n",
       "      <td>2608.000000</td>\n",
       "      <td>589.435410</td>\n",
       "      <td>27838.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>362.040945</td>\n",
       "      <td>10918.000000</td>\n",
       "      <td>6121.000000</td>\n",
       "      <td>5876.767423</td>\n",
       "      <td>19542.000000</td>\n",
       "      <td>28267.000000</td>\n",
       "      <td>5998.526417</td>\n",
       "      <td>23544.000000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>974.558557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                rank     frequency           pmw       PB_rank  PB_frequency  \\\n",
       "count   84251.000000  84251.000000  84251.000000  50613.000000  50613.000000   \n",
       "mean    96569.374215     61.648883      0.589307  65322.612451     27.371486   \n",
       "std     43838.040107    446.131664      4.264615  28576.195151    147.588499   \n",
       "min       142.000000      1.000000      0.009559    194.000000      1.000000   \n",
       "25%     61086.000000      2.000000      0.019118  42722.000000      1.000000   \n",
       "50%    100044.000000      6.000000      0.057355  69614.000000      4.000000   \n",
       "75%    134146.000000     23.000000      0.219859  98748.000000     14.000000   \n",
       "max    152442.000000  63392.000000    605.970096  98748.000000  11614.000000   \n",
       "\n",
       "             PB_pmw       PM_rank  PM_frequency        PM_pmw       PN_rank  \\\n",
       "count  50613.000000  24318.000000  24318.000000  24318.000000  14900.000000   \n",
       "mean       0.962074  31707.032939     18.169011      4.106387  20032.300872   \n",
       "std        5.187552  14808.067118     68.786971     15.546578   8077.661334   \n",
       "min        0.035149    129.000000      1.000000      0.226011     84.000000   \n",
       "25%        0.035149  19529.000000      1.000000      0.226011  13724.000000   \n",
       "50%        0.140595  34673.000000      3.000000      0.678032  21631.000000   \n",
       "75%        0.492083  48409.000000     10.000000      2.260105  27838.000000   \n",
       "max      408.217653  48409.000000   2608.000000    589.435410  27838.000000   \n",
       "\n",
       "       ...  LB_variable_pmw  OW_fixed_rank  OW_fixed_frequency  OW_fixed_pmw  \\\n",
       "count  ...     52380.000000    3291.000000         3291.000000   3291.000000   \n",
       "mean   ...         0.892448    8300.864175            9.368885      8.995059   \n",
       "std    ...         4.255047    2921.772221          110.032822    105.642428   \n",
       "min    ...         0.034688      21.000000            1.000000      0.960099   \n",
       "25%    ...         0.069376    6053.000000            1.000000      0.960099   \n",
       "50%    ...         0.138753    8692.000000            2.000000      1.920199   \n",
       "75%    ...         0.520323   10918.000000            5.000000      4.800496   \n",
       "max    ...       362.040945   10918.000000         6121.000000   5876.767423   \n",
       "\n",
       "       OW_variable_rank  OW_variable_frequency  OW_variable_pmw     core_rank  \\\n",
       "count       7028.000000            7028.000000      7028.000000  11172.000000   \n",
       "mean       14504.842629              20.088788         4.263032  17160.913892   \n",
       "std         5375.571082             348.209277        73.893323   6831.599040   \n",
       "min           19.000000               1.000000         0.212209     90.000000   \n",
       "25%        10881.000000               1.000000         0.212209  12560.000000   \n",
       "50%        16048.000000               2.000000         0.424419  17861.000000   \n",
       "75%        19542.000000               6.000000         1.273257  23544.000000   \n",
       "max        19542.000000           28267.000000      5998.526417  23544.000000   \n",
       "\n",
       "       core_frequency      core_pmw  \n",
       "count    11172.000000  11172.000000  \n",
       "mean         5.841568      5.320514  \n",
       "std         18.307100     16.674150  \n",
       "min          1.000000      0.910802  \n",
       "25%          1.000000      0.910802  \n",
       "50%          2.000000      1.821605  \n",
       "75%          4.000000      3.643210  \n",
       "max       1070.000000    974.558557  \n",
       "\n",
       "[8 rows x 75 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist.describe() # stats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "What I did before anything else was rename some columns and create a `cleanwordlist` so that I only had the data I needed. 80 columns is a lot, and I don't really know what a lot of it means. Luckily, all I really need is the Katakana word, the translation, and the frequency count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist.rename(columns={'lemma':'katakana', 'subLemma':'translation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanwordlist = wordlist[['katakana', 'translation', 'frequency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>percent</td>\n",
       "      <td>63392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>America</td>\n",
       "      <td>28243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ページ</td>\n",
       "      <td>page</td>\n",
       "      <td>24642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>センター</td>\n",
       "      <td>center</td>\n",
       "      <td>20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>サービス</td>\n",
       "      <td>service</td>\n",
       "      <td>16630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>システム</td>\n",
       "      <td>system</td>\n",
       "      <td>16458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>メートル</td>\n",
       "      <td>metre</td>\n",
       "      <td>15960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>テレビ</td>\n",
       "      <td>television</td>\n",
       "      <td>15644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>メール</td>\n",
       "      <td>mail</td>\n",
       "      <td>15589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  katakana translation  frequency\n",
       "0    パーセント     percent      63392\n",
       "1    トウキョウ         NaN      29804\n",
       "2     アメリカ     America      28243\n",
       "3      ページ        page      24642\n",
       "4     センター      center      20664\n",
       "5     サービス     service      16630\n",
       "6     システム      system      16458\n",
       "7     メートル       metre      15960\n",
       "8      テレビ  television      15644\n",
       "9      メール        mail      15589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice here that some translations are `NaN`. There are several of these in the dataframe, but the first one we see is トウキョウ (Tokyo, as in the city, which has no English equivalent). Let's explore some more of this `NaN` translation data to see if the other ones are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan = cleanwordlist['translation'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>トウキョウ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>オオサカ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>センチメートル</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>キョウト</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>エド</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   katakana translation  frequency\n",
       "1     トウキョウ         NaN      29804\n",
       "11     オオサカ         NaN      11572\n",
       "25  センチメートル         NaN       9060\n",
       "30     キョウト         NaN       8495\n",
       "38       エド         NaN       8100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notranslation = cleanwordlist[isnan]\n",
    "notranslation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words read \"Tokyo\", \"Oosaka\", \"Senchimeetoru\" (centimeter), \"Kyoto\", and \"Edo\". Four of these five words are places in Japan. Centimeter isn't, but I know from looking at the list of words in the Excel spreadsheet that センチ \"senchi\" is generally used as the word for centimeter, so it makes sense that there's no translation for it. Just to be safe, let's take a random sample of non-translated words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22904</td>\n",
       "      <td>ムネユキ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51165</td>\n",
       "      <td>リドルフィ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52428</td>\n",
       "      <td>カテプリ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18465</td>\n",
       "      <td>タクオ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14939</td>\n",
       "      <td>オオクノ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      katakana translation  frequency\n",
       "22904     ムネユキ         NaN         20\n",
       "51165    リドルフィ         NaN          4\n",
       "52428     カテプリ         NaN          3\n",
       "18465      タクオ         NaN         28\n",
       "14939     オオクノ         NaN         39"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notranslation.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words all seem to be names, judging by their translation and frequencies. Even if these words have meaning, I plan on filtering out words with lower frequency counts just to get rid of these names regardless, so even if I left null translations in, it wouldn't really affect what I'm trying to do here. Let's filter out the NaNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanwordlist = cleanwordlist.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>percent</td>\n",
       "      <td>63392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>America</td>\n",
       "      <td>28243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ページ</td>\n",
       "      <td>page</td>\n",
       "      <td>24642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>センター</td>\n",
       "      <td>center</td>\n",
       "      <td>20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>サービス</td>\n",
       "      <td>service</td>\n",
       "      <td>16630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  katakana translation  frequency\n",
       "0    パーセント     percent      63392\n",
       "2     アメリカ     America      28243\n",
       "3      ページ        page      24642\n",
       "4     センター      center      20664\n",
       "5     サービス     service      16630"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>30140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>113.244924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>690.539893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>63392.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "count  30140.000000\n",
       "mean     113.244924\n",
       "std      690.539893\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        9.000000\n",
       "75%       39.000000\n",
       "max    63392.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, just from getting rid of the null entries, we got rid of 50,000 words.   \n",
    "  \n",
    "Anyway, as I mentioned before, I'm going to drop any words that have an incredibly low frequency. There's no point in trying to recognize a word if there's a pretty good chance it won't be in there. The hard part here will be trying to figure out what the cutoff should be. Let's check some value counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       4484\n",
       "2       2944\n",
       "3       1937\n",
       "4       1492\n",
       "5       1286\n",
       "        ... \n",
       "1285       1\n",
       "1269       1\n",
       "3284       1\n",
       "5319       1\n",
       "2047       1\n",
       "Name: frequency, Length: 1394, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwordlist['frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's a lot of words that are only used a single-digit amount of times. I don't want to drop too many words, but I also don't want to make this harder than it has to be. We did see above that the majority of the words were used less than 23 times, but we don't want to include anything that's too irrelevant. Let's go with dropping anything less than a frequency of 75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalwordlist = cleanwordlist.loc[cleanwordlist['frequency'] >= 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>katakana</th>\n",
       "      <th>translation</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>パーセント</td>\n",
       "      <td>percent</td>\n",
       "      <td>63392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>アメリカ</td>\n",
       "      <td>America</td>\n",
       "      <td>28243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ページ</td>\n",
       "      <td>page</td>\n",
       "      <td>24642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>センター</td>\n",
       "      <td>center</td>\n",
       "      <td>20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>サービス</td>\n",
       "      <td>service</td>\n",
       "      <td>16630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  katakana translation  frequency\n",
       "0    パーセント     percent      63392\n",
       "2     アメリカ     America      28243\n",
       "3      ページ        page      24642\n",
       "4     センター      center      20664\n",
       "5     サービス     service      16630"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalwordlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5192.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>595.130008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1576.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>63392.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "count   5192.000000\n",
       "mean     595.130008\n",
       "std     1576.927800\n",
       "min       75.000000\n",
       "25%      115.000000\n",
       "50%      202.000000\n",
       "75%      475.000000\n",
       "max    63392.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalwordlist.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered out a lot of data... we went from around 80,000 entries to around 5,000. This makes sense, considering the overwhelming amount of nulls and rarely-used words there were. If I find that I have to go back and make this more inclusive, I will. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Reading (Conversation Data)\n",
    "This part will be a little more challenging to work with. In the Nagoya Conversation Corpus, there are just text files with some annotations in them. There is data on the corpus website that specifies age groups of participants, but I'm going to need to compile a spreadsheet file myself. I'm not sure what data I'll need, so I'm importing two spreadsheets that I created; one based on the participants and one based on the text files. The participant spreadsheet probably won't need any modifying, but I'll need to import the content into the spreadsheet based on the text files. How I use these spreadsheets and whether they'll be any use at all is hard to say at this point, as I haven't actually started working with it yet, but it's a starting point. Since there aren't many text files or participants (both less than 200, creating a manual spreadsheet won't take all that long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "byparticipant = pd.read_csv('../privdata/participantbased.csv')\n",
    "byfile = pd.read_csv('../privdata/filebased.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>age</th>\n",
       "      <th>appears_count</th>\n",
       "      <th>appears_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F001</td>\n",
       "      <td>Early 20s</td>\n",
       "      <td>5</td>\n",
       "      <td>data105.txt data086.txt data076.txt data075.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F002</td>\n",
       "      <td>Late 60's</td>\n",
       "      <td>3</td>\n",
       "      <td>data033.txt data032.txt data031.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>F003</td>\n",
       "      <td>Late 80's</td>\n",
       "      <td>1</td>\n",
       "      <td>data129.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F004</td>\n",
       "      <td>Late 20's</td>\n",
       "      <td>14</td>\n",
       "      <td>data096.txt data094.txt data092.txt data082.tx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>F005</td>\n",
       "      <td>Late 20's</td>\n",
       "      <td>3</td>\n",
       "      <td>data052.txt data023.txt data015.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant        age  appears_count  \\\n",
       "0        F001  Early 20s              5   \n",
       "1        F002  Late 60's              3   \n",
       "2        F003  Late 80's              1   \n",
       "3        F004  Late 20's             14   \n",
       "4        F005  Late 20's              3   \n",
       "\n",
       "                                          appears_in  \n",
       "0  data105.txt data086.txt data076.txt data075.tx...  \n",
       "1                data033.txt data032.txt data031.txt  \n",
       "2                                        data129.txt  \n",
       "3  data096.txt data094.txt data092.txt data082.tx...  \n",
       "4                data052.txt data023.txt data015.txt  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byparticipant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data002.txt</td>\n",
       "      <td>F107 F023 F128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data003.txt</td>\n",
       "      <td>F033 F056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data004.txt</td>\n",
       "      <td>M018 F128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data005.txt</td>\n",
       "      <td>M023 F128 F116 M026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants\n",
       "0  data001.txt  F107 F023 M023 F128\n",
       "1  data002.txt       F107 F023 F128\n",
       "2  data003.txt            F033 F056\n",
       "3  data004.txt            M018 F128\n",
       "4  data005.txt  M023 F128 F116 M026"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on here. I don't have a lot to do with this data since we're still just in the gathering and cleaning stage, and I did lots of this manually for the sake of time and ease. There may have been a more efficient way to create the spreadsheets with code, but due to the nature of the text files, I opted for a manual spreadsheet creation.  \n",
    "  \n",
    "What I do need to do, though, is add the actual content to the `byfile` dataframe. Let's do that real quick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../privdata/nucc\\\\data001.txt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = glob.glob('../privdata/nucc/*.txt')\n",
    "fname[0] #did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def readtxt(fn):\\n    f = open(glob.glob('../privdata/nucc/' + fn)[0])\\n    text = f.read()\\n    f.close()\\n    return text\\n\\nbyfile['content'] = byfile['file'].apply(readtxt)\\n\\nbyfile.head()\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def readtxt(fn):\n",
    "    f = open(glob.glob('../privdata/nucc/' + fn)[0])\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "byfile['content'] = byfile['file'].apply(readtxt)\n",
    "\n",
    "byfile.head()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string I've got above here is where I'm running into an issue with encoding. Python doesn't like the fact that I have non-ASCII characters in the files and doesn't want to read them in. Whether or not I will be able to fix this before the progress report 1 deadline is really up for debate, but if I don't figure it out, I'll investigate further post-due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROGRESS REPORT 2 START (Due 3/23/2020 [extended])\n",
    "### Some notes: Responses to comments/feedback, and where I'm trying to go from here\n",
    "I'm glad I seem to have gotten a lot done in the last report! There was a lot of simple stuff going on there, though, so I'm going to take my time a bit on the next stuff since it's going to involve more than just dropping columns.\n",
    "  \n",
    "  \n",
    "Re: the columns I dropped - most of the 80 columns were just different levels of frequencies and arbitrary ranks. For now, I'm going to leave it be, but if I find I need more analysis, it won't be too lengthy of a process to go back and include some columns in the word list since I won't be doing too much manipulating for the word list. \n",
    "  \n",
    "I also freehanded the number 75 for the cutoff, but I took a look at what data I had left and saw that the words that were used 75 times or less were proper nouns that I doubt would have come up in casual conversation. The word list is really there just to find Katakana words since there aren't any word bounds in Japanese. I'd just count the number of individual Katakana characters, but onomatopoeia words are also written in Katakana and I want to avoid countig those. Again, if I need to go back and manipulate according to what my analysis needs, it isn't that lengthy of a process. \n",
    "  \n",
    "  \n",
    "I will probably just share a sample of my found data, rather than the whole thing, just to be safe regarding the licenses for the data I'm using. The code for how to get there will be public, so there will be directions to follow (or deviate from) on how to get where I got. \n",
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "`filebased` and `participantbased` are just files I created on the fly, more like an \"in case I need it later\" deal. I now know what I'm going to use them for, but I'll get into that later. \n",
    "  \n",
    "Anyway, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the ASCII Issue\n",
    "I can't really move forward with anything until I get rid of that ASCII issue I had in the first progress report, so I'm going to start there. I'm gonna try just adding the `encoding` parameter to my function from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "      <td>＠データ１（約３５分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data002.txt</td>\n",
       "      <td>F107 F023 F128</td>\n",
       "      <td>＠データ２（６０分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data003.txt</td>\n",
       "      <td>F033 F056</td>\n",
       "      <td>＠データ０３（４３分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（某大から所属...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data004.txt</td>\n",
       "      <td>M018 F128</td>\n",
       "      <td>＠データ０４（３５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（知立駅より西...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data005.txt</td>\n",
       "      <td>M023 F128 F116 M026</td>\n",
       "      <td>＠データ０５（５５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：M023の自宅\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants  \\\n",
       "0  data001.txt  F107 F023 M023 F128   \n",
       "1  data002.txt       F107 F023 F128   \n",
       "2  data003.txt            F033 F056   \n",
       "3  data004.txt            M018 F128   \n",
       "4  data005.txt  M023 F128 F116 M026   \n",
       "\n",
       "                                             content  \n",
       "0  ＠データ１（約３５分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラ...  \n",
       "1  ＠データ２（６０分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン...  \n",
       "2  ＠データ０３（４３分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（某大から所属...  \n",
       "3  ＠データ０４（３５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：車中（知立駅より西...  \n",
       "4  ＠データ０５（５５分）\\n＠収集年月日：２００１年１０月２３日\\n＠場所：M023の自宅\\n...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readtxt(fn):\n",
    "    f = open(glob.glob('../privdata/nucc/' + fn)[0], encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "byfile['content'] = byfile['file'].apply(readtxt)\n",
    "\n",
    "byfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that worked! What a short heading. \n",
    "\n",
    "### Cleaning up the content\n",
    "The end goal here is to extract each participant's lines and put them into `participantbased` so we can analyze how many Katakana words each participant used, rather than how many appear in each file, like I first intended to. Let's look at the contents of a file, because it looks like there's some sort of documentation going on (specifying the length of the conversation as 分 is the character for minutes, the date it took place, as well as the participants). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'＠データ１（約３５分）\\n＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン\\n＠参加者F107：女性３０代後半、愛知県幡豆郡出身、愛知県幡豆郡在住\\n＠参加者F023：女性４０代後半、岐阜県出身、愛知県幡豆郡在住\\n＠参加者M023：男性２０代前半、愛知県西尾市出身、西尾市在住\\n＠参加者F128：女性２０代前半、愛知県西尾市出身、西尾市在住\\n＠参加者の関係：英会話教室の友人\\nF107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁の上を歩いても１時間ぐらいですよね。\\nF023：１時間かからないぐらいだね。\\n４、５０分で。\\nF107：そうそう。\\nほいでさあ、ずっと歩いていたんだけど、そうすと上から、なんか町の中が見れるじゃん。\\nあるよね。\\nほいでさあ、なんか途中でワンちゃんに会ったんだね。\\n（ふーん）散歩をしてるワンちゃんに会ったんだ。\\nF023：城壁の上をやっぱ観光客なんだけどワンちゃん連れてきてる人たち結構多くて。\\nF107：で、こう、そのワンちゃんと２人を、なに、お父さんとお母さんと歩いて、ワンちゃんに会ったんだ。\\n途中で。\\nあワンちゃ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyfile = byfile['content'].iloc[0]\n",
    "toyfile[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not pleasant to look at in the slightest, and it's even worse without the [:500]. Any line beginning with an @ symbol is documentation, so let's find a way to remove that first. I know that there will always be one line at the end with `@END`, but the amount of documentation at the beginning of the files varies from file to file, because different files have different numbers of participants. I'm thinking what I can do is remove the last line of each file first, and then develop a method that removes the first x lines of documentation, so we start with an M/FXXX every time.   \n",
    "  \n",
    "Getting rid of `@END` is easy, because it's always the last few characters of the file. We can just do something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'かこれシャッターチャンスがさ、逃すん＜笑い＞だよね。\\n遅いもんでさあ。\\n反応が。\\n＠ＥＮＤ\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before removal\n",
    "toyfile[19500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing last five characters\n",
    "toyfile = toyfile[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'かこれシャッターチャンスがさ、逃すん＜笑い＞だよね。\\n遅いもんでさあ。\\n反応が。\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after removal \n",
    "toyfile[19500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There! That was pretty easy, so let's do it for every file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for x in byfile['file']: \n",
    "    cur = byfile['content'].iloc[ct]\n",
    "    cur = cur[:-5]\n",
    "    byfile['content'].iloc[ct] = cur\n",
    "    ct = ct+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'149：そうそうそうそう。\\n何これ、みたいな、（うん）これが文字、とか思うよね。\\nF069：ある。\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byfile['content'].iloc[10][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! I double checked the above line with a few other `iloc`s and compared them to their conversation file, and everything checked out.   \n",
    "  \n",
    "Now comes the harder part; removing the first x lines of documentation. This varies from document to document, so it's not as simple as removing the last five characters. Let's play with `toyfile` again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyfile.startswith('＠') # the japanese keyboard has a funky version of @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = toyfile.index('\\n')    \n",
    "ind = ind + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyfile = toyfile[ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'＠収集年月日：２００１年１０月１６日\\n＠場所：ファミリーレストラン\\n＠参加者F107：女性３０代後半、愛知県幡豆郡出身、愛知県幡豆郡在住\\n＠参加者F023：女性４０代後半、岐阜県出身、愛知県幡豆郡在住\\n＠参加者M023：男性２０代前半、愛知県西尾市出身、西尾市在住\\n＠参加者F128：女性２０代前半、愛知県西尾市出身、西尾市在住\\n＠参加者の関係：英会話教室の友人\\nF107：＊＊＊の町というのはちいち'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyfile[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're on to something here. Since we only get this documentation at the beginning, there's no need to go past the last instance of ＠, which is more efficient anyway on the program's part. We'll just need to add one to ind so it gets rid of that empty newline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "while toyfile.startswith('＠'):\n",
    "    ind = toyfile.index('\\n')\n",
    "    ind = ind + 1\n",
    "    toyfile = toyfile[ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁の上を歩いても１時間ぐらいですよね。\\nF023：１時間かからないぐらいだね。\\n４、５０分で。\\nF107：そうそう。\\nほいでさあ、ずっと歩いていたんだけど、そうすと上から、なんか町の中が見れるじゃん。\\nあるよね。\\nほいでさあ、なんか途中でワンちゃんに会ったんだね。\\n（ふーん）散歩をしてるワンちゃんに会ったんだ'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyfile[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Now let's apply it to everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for x in byfile['file']: \n",
    "    cur = byfile['content'].iloc[ct]\n",
    "    while cur.startswith('＠'):\n",
    "        ind = cur.index('\\n')\n",
    "        ind = ind + 1\n",
    "        cur = cur[ind:]\n",
    "    byfile['content'].iloc[ct] = cur\n",
    "    ct = ct+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "      <td>F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data002.txt</td>\n",
       "      <td>F107 F023 F128</td>\n",
       "      <td>％ｃｏｍ：F023は出身地に２６歳まで居住。\\nF107：今度はーイギリスにもアメリカと同様...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data003.txt</td>\n",
       "      <td>F033 F056</td>\n",
       "      <td>F033：倒れちゃう。\\nF056：いきなり倒れた。\\nF033：どうしよう。あっ、この間に...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data004.txt</td>\n",
       "      <td>M018 F128</td>\n",
       "      <td>F128：いや、別にいいよ。\\nローソンでいいやろ。\\nちょっと倒していい、これ。\\nどうよ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data005.txt</td>\n",
       "      <td>M023 F128 F116 M026</td>\n",
       "      <td>％ｃｏｍ：F116はずっと愛知県に居住している。現住所には２４年間居住。M023は沖縄に４年...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants  \\\n",
       "0  data001.txt  F107 F023 M023 F128   \n",
       "1  data002.txt       F107 F023 F128   \n",
       "2  data003.txt            F033 F056   \n",
       "3  data004.txt            M018 F128   \n",
       "4  data005.txt  M023 F128 F116 M026   \n",
       "\n",
       "                                             content  \n",
       "0  F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...  \n",
       "1  ％ｃｏｍ：F023は出身地に２６歳まで居住。\\nF107：今度はーイギリスにもアメリカと同様...  \n",
       "2  F033：倒れちゃう。\\nF056：いきなり倒れた。\\nF033：どうしよう。あっ、この間に...  \n",
       "3  F128：いや、別にいいよ。\\nローソンでいいやろ。\\nちょっと倒していい、これ。\\nどうよ...  \n",
       "4  ％ｃｏｍ：F116はずっと愛知県に居住している。現住所には２４年間居住。M023は沖縄に４年...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, another issue: that ％ｃｏｍ business. These ％ｃｏｍ lines are to indicate whether someone has moved places of residency. This isn't relevant to what I'm trying to do, and it's not conversation data, so we can do the same thing we did with the funky @ symbol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for x in byfile['file']: \n",
    "    cur = byfile['content'].iloc[ct]\n",
    "    while cur.startswith('％ｃｏｍ'):\n",
    "        ind = cur.index('\\n')\n",
    "        ind = ind + 1\n",
    "        cur = cur[ind:]\n",
    "    byfile['content'].iloc[ct] = cur\n",
    "    ct = ct+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "      <td>F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data002.txt</td>\n",
       "      <td>F107 F023 F128</td>\n",
       "      <td>F107：今度はーイギリスにもアメリカと同様のテロが起こるだろうって言ったんだってよ。\\nF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data003.txt</td>\n",
       "      <td>F033 F056</td>\n",
       "      <td>F033：倒れちゃう。\\nF056：いきなり倒れた。\\nF033：どうしよう。あっ、この間に...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data004.txt</td>\n",
       "      <td>M018 F128</td>\n",
       "      <td>F128：いや、別にいいよ。\\nローソンでいいやろ。\\nちょっと倒していい、これ。\\nどうよ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data005.txt</td>\n",
       "      <td>M023 F128 F116 M026</td>\n",
       "      <td>F128：来てたときによく貸してもらったやつだ。\\nM023：そう、そんな感じのとこ。\\nF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants  \\\n",
       "0  data001.txt  F107 F023 M023 F128   \n",
       "1  data002.txt       F107 F023 F128   \n",
       "2  data003.txt            F033 F056   \n",
       "3  data004.txt            M018 F128   \n",
       "4  data005.txt  M023 F128 F116 M026   \n",
       "\n",
       "                                             content  \n",
       "0  F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...  \n",
       "1  F107：今度はーイギリスにもアメリカと同様のテロが起こるだろうって言ったんだってよ。\\nF...  \n",
       "2  F033：倒れちゃう。\\nF056：いきなり倒れた。\\nF033：どうしよう。あっ、この間に...  \n",
       "3  F128：いや、別にいいよ。\\nローソンでいいやろ。\\nちょっと倒していい、これ。\\nどうよ...  \n",
       "4  F128：来てたときによく貸してもらったやつだ。\\nM023：そう、そんな感じのとこ。\\nF...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaaand that's the last of the documentation! The harder part comes in when we have to parse each line of speech so that every participant is associated with what exactly they say.  \n",
    "  \n",
    "Next what I'll do is parse the content by line and parse the participants for the sake of assigning them to their respective slots in the participant dataframe. Like I've been doing, rather than trying to edit my dataframes and risking having to restart my kernel every time I do something wrong, I'm going to work with `toyfile` and make toy dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "toybyfile = pd.read_csv('../samples/toybyfile.csv')\n",
    "toybypar = pd.read_csv('../samples/toybypar.csv')\n",
    "toybyfile = toybyfile.fillna(toyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>F107 F023 M023 F128</td>\n",
       "      <td>F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file         participants  \\\n",
       "0  data001.txt  F107 F023 M023 F128   \n",
       "\n",
       "                                             content  \n",
       "0  F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toybyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F128</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant  content\n",
       "0       F107       NaN\n",
       "1        F023      NaN\n",
       "2        M023      NaN\n",
       "3        F128      NaN"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toybypar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now that we have that done, let's experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "toybyfile['participants'][0] = toybyfile['participants'][0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>[F107, F023, M023, F128]</td>\n",
       "      <td>F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file              participants  \\\n",
       "0  data001.txt  [F107, F023, M023, F128]   \n",
       "\n",
       "                                             content  \n",
       "0  F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toybyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "toybyfile['content'][0] = toybyfile['content'][0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>participants</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data001.txt</td>\n",
       "      <td>[F107, F023, M023, F128]</td>\n",
       "      <td>[F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file              participants  \\\n",
       "0  data001.txt  [F107, F023, M023, F128]   \n",
       "\n",
       "                                             content  \n",
       "0  [F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toybyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're set as far as splitting goes. Note, though, that not every line starts with a new speaker; it's just a continuation of the last line. We need to take this into account when we're sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = toybyfile['participants'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pars[0] \n",
    "p2 = pars[1]\n",
    "p3 = pars[2]\n",
    "p4 = pars[3] # we'll come back to these "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, I'm going to merge previous, \"speakerless\" lines with the previous lines, because that new line is just another sentence spoken by the speaker specified in previous lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpcontent = toybyfile['content'][0]\n",
    "lencontent = len(tmpcontent)\n",
    "count = 1\n",
    "while count < lencontent:\n",
    "    if (not tmpcontent[count].startswith('M') and not tmpcontent[count].startswith('F')):\n",
    "        tmpcontent[count-1] = tmpcontent[count-1] + tmpcontent[count]\n",
    "        del tmpcontent[count]\n",
    "        lencontent = len(tmpcontent)\n",
    "    else:\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F107：＊＊＊の町というのはちいちゃくって、城壁がこう町全体をぐるっと回ってて、それが城壁の上を歩いても１時間ぐらいですよね。', 'F023：１時間かからないぐらいだね。４、５０分で。', 'F107：そうそう。ほいでさあ、ずっと歩いていたんだけど、そうすと上から、なんか町の中が見れるじゃん。あるよね。ほいでさあ、なんか途中でワンちゃんに会ったんだね。（ふーん）散歩をしてるワンちゃんに会ったんだ。', 'F023：城壁の上をやっぱ観光客なんだけどワンちゃん連れてきてる人たち結構多くて。', 'F107：で、こう、そのワンちゃんと２人を、なに、お父さんとお母さんと歩いて、ワンちゃんに会ったんだ。途中で。あワンちゃーんとか言ってなでて、ほいで、この人たちはこっち行って、あたしらこっち行ったじゃん。ずうーとこうやって回ってきてるの。また会っちゃって。ここで。そうしたら。', 'F128：おー、そら地球はやっぱり丸かったみたいだね。\\u3000', 'F107：そうしたらそのワンちゃんがなんかか喜んじゃって、で、あたしの方に走ってきて、とびついてきちゃってさ。別にあたしさあ、別にさっきなでただけなのにさあ、なんかすごーいなつかれちゃってね。\\u3000', 'F023：さっきね、別に、そんなになでてもいないんだよ。', 'F107：よしよしって言っただけなのに。', 'F023：あらワンちゃんだーとか言ってすれ違ったんだよ。普通に。それでその次のとき、向こうの方からはーっといってかけてくるじゃん。']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpcontent[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: \\u3000 is just a longer space used in Japanese text; its inclusion or exclusion won't have any impact on what I'm trying to do \n",
    "  \n",
    "  \n",
    "That seems to have worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
